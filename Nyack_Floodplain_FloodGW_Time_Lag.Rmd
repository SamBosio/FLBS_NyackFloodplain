---
title: "Floods Time Lag Regression"
author: "Amalia Handler"
date: "3/16/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---
This script is used to investigate the influence of Middle Fork of the Flathead River floods on the Nyack Floodplain. The data (temperature, dissolved oxygen, conductivity, and water level) include six groundwater wells and three surface water wells.

Note that the data used here has been QAQC'ed through removal of bad data associated with sensor error and servicing using the script titled "Nyack_Floodplain_Water_QAQC.Rmd" available on Amalia Handler's GitHub repository amaliahandler/FLBS_NyackFloodplain.


```{r}
# Load packages

# Load data
options(stringsAsFactors = FALSE)

cond  <- read.csv('./Data/Nyack_cond_cleaned.csv', header = TRUE)
do    <- read.csv('./Data/Nyack_do_cleaned.csv', header = TRUE)
level <- read.csv('./Data/Nyack_level_cleaned.csv', header = TRUE)

```


Get a few test periods of flood data to test out isolating floods


```{r}
# Water level flood data
# Bad data: value = -999
# Questionable data flags: 1 (questionable), 2 (silt in well)

# Remove questionable and bad data
level <- level[-which(level$level_m == -999 | level$level_m_flag == 1 | level$level_m_flag == 2),]

# Convert datetime to POSIX variable type
level$datetime <- as.POSIXct(level$datetime, tz = "MST")

# Rank groundwater wells by distance from the Middle Fork of the Flathead River
# The water level on the Middle Fork (CASC sensor) receives a rank of 1
sensor <- c(6,9,5,7,4,1,3)
rank   <- c(1:7)
rank   <- data.frame(sensor, rank)

level  <- merge(level, rank, by.x = "sensor_number", by.y = "sensor")

# Subset the water level data to only include the sensors for this analysis
level <- level[level$sensor_number %in% sensor,]

# Break the df into a list based on rank
level <- split(level, f = as.factor(level$rank))

# Make the names the sensor number
names(level) <- sensor

# Set par
par(mfrow = c(1,1), mar = c(4,4,1,1))

# Testing
start_date <- "2017-03-08"
end_date <- "2017-03-25"

flood_finder <- function(start_date, end_date){
  # Define period of time containing flood
  start <- as.POSIXct(start_date)
  end   <- as.POSIXct(end_date)
  
  # Subset water level data to these dates
  sub_lvl <- lapply(level, function(x){x[x$datetime >= start & x$datetime <= end,]})

  # Plot the water level at CASC to see if baseflow prior to flood, rising limb, and falling limb are included in the subset
  with(sub_lvl[[1]], plot(datetime, level_m, pch = 19))
  
  return(sub_lvl)
}

# Potential sample periods with example floods
tmp1 <- flood_finder("2017-03-08", "2017-03-18")
tmp2 <- flood_finder("2016-10-06", "2016-10-14")
tmp3 <- flood_finder("2014-04-01", "2014-04-18")
tmp4 <- flood_finder("2014-04-25", "2014-05-10")
tmp5 <- flood_finder("2014-11-26", "2014-12-10")
tmp6 <- flood_finder("2017-03-18", "2017-03-30")

```


Experiment with the Savitzky-Golay smoothing filters to detect floods in the main stem


```{r}
detach("package:dplyr", unload = TRUE)
library(signal)

# Savitsky-Golay filter settings
# p - order of polynomial line fit
# n - number of observations in the window (must be odd)
# m - derivative order to be returned of the fitted line
sg <- sgolay(p = 3, n = 9, m = 0)
sgd <- sgolay(p = 3, n = 9, m = 1)
sga <- sgolay(p = 3, n = 9, m = 2)

t_smooth <- filter(sg, tmp2[[1]]$level_m)
t_derv   <- filter(sgd, tmp2[[1]]$level_m)
t_acc    <- filter(sga, tmp2[[1]]$level_m)

# Function for applying the filter to the example data
# Try setting a TH for the derivative
# Try looking at the distribution of the derivitives
# Try flagging values above the some percentile slope
# Try calculating the 2nd derivative to calculate acceleration
# Try applying a secondary filter where if 8/10 observations are above the TH, then flag as a flood

# Set the TH parameters for flood rising limb detection
p <- 0.85 # percentile threshold for first derivative
w <- 9    # window of observation to consider
n <- 6    # number of observations that must meet the threshold

# Set the SG filter parameters
sg <- sgolay(p = 3, n = 17, m = 0)
sgd <- sgolay(p = 3, n = 17, m = 1)

# Set par
par(mfrow = c(2,1), oma = c(0.1,0.1,0.1,0.1), mar = c(2,4,0.3,0.5))

# Function testing
sw_gw_list <- tmp2
start_flood_obs <- 165

# Function identifies and isolates rising limb of flood base on the threshold for the main stem of the stream, then subsets data from the groundwater for the same period plus 24 hours.
plot_sg <- function(sw_gw_list, start_flood_obs){
  # Filter the mainstem water trace based on the Savitsky-Golay filter
  df <- sw_gw_list[[1]]
  t_smooth <- filter(sg, df$level_m)
  t_derv   <- filter(sgd, df$level_m)
 
  q <- as.numeric(quantile(t_derv, p))

  # Create flag based on a moving look at observations
  flag <- sapply(((w-1)/2+1):(length(t_derv)-((w-1)/2+1)), function(i){
    above_q <- t_derv[(i-((w-1)/2)):(i+((w-1)/2))] > q
    n_true  <- length(above_q[above_q])
    ifelse(n_true >= n, 1, 0)
  })
  
  # Make a vector of flag data
  flag <- c(rep(NA, ((w-1)/2)), flag, rep(NA, ((w-1)/2+1)))
  
  # Add the flag to the flood water level
  df$flag <- flag
  df$derv <- t_derv
  
  # Get the flag to include the peak of the flood
  # Subset to oberservations that come after the first flag
  first_flag <- min(df$datetime[which(df$flag == 1)])
  sub <- df[df$datetime >= first_flag,]
  
 # After all flagged observations, find the place where the first derivative crosses zero
  peak_flow <- min(sub$datetime[which(sub$derv < 0)]) - 60*60
  
  # Flag all observations between the first flag and peak flow should be flagged as part of the rising limb of the flood
  sub$flag[sub$datetime >= first_flag & sub$datetime <= peak_flow] <- 1
  
  # Add this flag back to the main df
  df$flag[df$datetime >= first_flag] <- sub$flag
  
  # Plot it out
  y <- df$level_m
  x <- df$datetime
  flag <- df$flag
  derv <- df$derv
  
  # Plot data (raw)
  plot(x, y, xlab = '', ylab = "Water Level (m)", xaxt = 'n')
  # points(x, t_smooth, col = "blue", pch = 19, cex = 1)
  abline(v = x[start_flood_obs])
  # Flag points that meet the threshold
  points(x[flag == 1], t_smooth[flag == 1], pch = 19, col = "blue")
  
  # Plot 1st derivative (slope)
  plot(x, derv, ylab = "1st Derivative", xlab = '')
  abline(h = 0, col = "blue")
  abline(v = x[start_flood_obs])
  points(x[flag == 1], derv[flag == 1], col = "blue")
  abline(h = q, col = "blue")
  
  # Return just the rising limb of the flood plus 24 hours for the GW
  rising_limb <- lapply(sw_gw_list, function(x){
    x[x$datetime >= first_flag & x$datetime <= (peak_flow + 24*60*60),]
  })
  # rising_limb <- df[which(df$flag == 1),]
  return(rising_limb)
}

# Need to alter the function such that the output is the rising limb in the surface water plus the same datetimes in each of the groundwater wells plus 24 hours (since this is the cap I am applying to lag times for now)

r1 <- plot_sg(tmp1, 160)
r2 <- plot_sg(tmp2, 75)
r3 <- plot_sg(tmp3, 175)
r4 <- plot_sg(tmp4, 140)
r5 <- plot_sg(tmp5, 25)
r6 <- plot_sg(tmp6, 10)

```

Stuff learned so far

Identifying floods is really hard

Using a threshold percentile value for the first derivative does a decent job at identifying the start of the flood. This method however flags many observations that are not included in the flood (small changes in water level before and after). It also only captures the rising limb of the flood. 

Might consider experimenting with a moving average approach to identify where the flood start occurs. Same for when a flood ends: If the slope stays the same for move than i observations, then it's just flood recession and not longer an active flood. Maybe consider not qualitying the end of the flood, because that is a lot harder and more nuanced. Getting the start time and the peak time will have to do for now (and will enable me to get a questions about the magnitude of disturbance).

Looks like using a moving average is a decent approach, but is very sensitive to the window of time included in the analysis since it uses a pecentile threshold. Found that using a window of 9 observations of the 1st derivative and saying that if at least 6 of these is above the 85th percentile of 1st derivative values, then the center observation of the window is flagged as part of the flood rising limb. 

Using the first derivative to identify the peak of the flood works well. 

The second derivative is not super informative and does not add much info to the analysis.


############################################################


Now play around with building linear models with different lags




Run time lag models for just the rising limb of the flood

```{r}
# Just the rising limb

# Note that the signal package interfers with dplyr and so has to be removed before proceeding
detach("package:signal", unload = TRUE)
library(dplyr)

# Run linear models with varying time lags for river water level
# Output should be a measure of RMSE by well and lag time
# Or maybe just return the lag that had the smallest RMSE

# Site names for wells
wells <- c("HA02", "HA12", "HA15", "HA10", "HA08", "HA07")
id    <- c(1:5)

# Testing
ls <- r1
i  <- 6
l  <- 3

find_lag <- function(ls){
  # Iterate over the gw wells, elements 2:7 in each list
  well_lags <- lapply(c(2:7), function(i){
    # The number of lags to be tried
    lags <- c(0:24)
    
    sw <- ls[[1]]
    gw <- ls[[i]]
    
    # Ensure that the datetimes of obervations lines up between sw and gw dataframes
    sw_gw <- merge(sw[, c("datetime", "level_m")], gw[, c("datetime", "level_m")], by = "datetime", all = TRUE)
    num_row <- nrow(sw_gw)
    
    # Isolate surface water rising limb
    end_rising_limb <- max(sw_gw$datetime) - 24*60*60
    sw_rising_limb  <- sw_gw[sw_gw$datetime <= end_rising_limb, 2]
    
    gw_lag <- sapply(lags, function(l){
      # Run the linear model, predict GW level from river water level
      # Always use the same window of observations associated with the surface water rising limb of the flood. Only the window of observations from the groundwater will change as the size of the lag increases.
      # Rising limb of the SW is always going to be the daterange of the df minus 24 hours

      # Isolate GW window
      gw_srt <- sw_gw$datetime[1] + l * 60*60
      gw_end <- end_rising_limb + l * 60*60
      gw_win <- sw_gw[sw_gw$datetime >= gw_srt & sw_gw$datetime <= gw_end, 3]
      lm_lag <- lm(gw_win ~ sw_rising_limb)
      
      # Extract the model R^2
      r_squ <- round(summary(lm_lag)$r.squared, 3)
      return(r_squ)
    })
    
    # Merge together results for the 25 lags (include lag = 0)
    #well_lags <- do.call(max, gw_lag)
    #df <- tibble(well_lags)
    #return(df)
    best_fit <- max(gw_lag)
    best_lag <- lags[match(best_fit, gw_lag)]
    well <- wells[i-1]
    df <- data.frame(well, lag = best_lag, r_squ = best_fit, rank = i-1)
    return(df)
  })
  
  # Merge columns together to get the R square for each well
  # all_well_lags <- do.call(cbind, well_lags)
  all_well_lags <- do.call(rbind, well_lags)
  
  #rownames(all_well_lags) <- lags
  
  #colnames(all_well_lags) <- c("HA02", "HA12", "HA15", "HA10", "HA08", "HA07")
  
  return(all_well_lags)
}

lag1 <- find_lag(tmp1)
lag2 <- find_lag(tmp2)
lag3 <- find_lag(tmp3)
lag4 <- find_lag(tmp4)
lag5 <- find_lag(tmp5)
lag6 <- find_lag(tmp6)

all_lag <- rbind(lag1, lag2, lag3, lag4, lag5, lag6)

mean_lag <- all_lag %>%
  group_by(rank) %>%
  summarise(mean_lag = mean(lag),
            sd_lag = sd(lag))

# Make plot
png("./Figures/Flood Figures/GW Mean Lag Time_2020-03-20.png", units = "in", res = 150, width = 7, height = 4)
par(mfrow = c(1,1), mar = c(4,6,1,1), oma = c(0,0,0,0))
plot(mean_lag$rank, mean_lag$mean_lag, pch = 19, bg = "grey", xlab = "", ylab = "",
        xlim = c(0.5, 6.5), ylim = c(-2,8), xaxt = "n", cex = 1.5)
with(mean_lag, arrows(rank, mean_lag-sd_lag, rank, mean_lag+sd_lag, code = 3, angle = 90))
axis(1, at=c(1:6), labels=c('1: HA02','2: HA12','3: HA15','4: HA10','5: HA08','6: HA07'),
     las=1, adj=0)
title(ylab = "Lag (Mean +/- SE) Relative to Main Stem (hr)", line = 2.5)
title(xlab = "Rank Distance from Main Stem", line = 4.5)
dev.off()


```


Relate the whole window in SW to GW level data

```{r}
detach("package:signal", unload = TRUE)
library(dplyr)

# Run linear models with varying time lags for river water level
# Output should be a measure of RMSE by well and lag time
# Or maybe just return the lag that had the smallest RMSE

# Site names for wells
wells <- c("HA02", "HA12", "HA15", "HA10", "HA08", "HA07")
id    <- c(1:5)

# Testing
ls <- tmp1
i  <- 6
l  <- 3

find_lag <- function(ls){
  # Iterate over the gw wells, elements 2:7 in each list
  well_lags <- lapply(c(2:7), function(i){
    # The number of lags to be tried
    lags <- c(0:24)
    
    sw <- ls[[1]]
    gw <- ls[[i]]
    
    # Ensure that the datetimes of obervations lines up between sw and gw dataframes
    merged_data <- merge(sw[, c("datetime", "level_m")], gw[, c("datetime", "level_m")], by = "datetime", all = TRUE)
    num_row <- nrow(merged_data)
    
    gw_lag <- sapply(lags, function(l){
      # Run the linear model, predict GW level from river water level
      lm_lag <- lm(merged_data[1:(num_row-l), 3] ~ merged_data[(1+l):num_row, 2])
      
      # Extract the model R^2
      r_squ <- round(summary(lm_lag)$r.squared, 3)
      #df <- tibble(r_squ)
      return(r_squ)
    })
    
    # Merge together results for the 25 lags (include lag = 0)
    #well_lags <- do.call(max, gw_lag)
    #df <- tibble(well_lags)
    #return(df)
    best_fit <- max(gw_lag)
    best_lag <- lags[match(best_fit, gw_lag)]
    well <- wells[i-1]
    df <- data.frame(well, lag = best_lag, r_squ = best_fit, rank = i-1)
    return(df)
  })
  
  # Merge columns together to get the R square for each well
  # all_well_lags <- do.call(cbind, well_lags)
  all_well_lags <- do.call(rbind, well_lags)
  
  #rownames(all_well_lags) <- lags
  
  #colnames(all_well_lags) <- c("HA02", "HA12", "HA15", "HA10", "HA08", "HA07")
  
  return(all_well_lags)
}

lag1 <- find_lag(tmp1)
lag2 <- find_lag(tmp2)
lag3 <- find_lag(tmp3)
lag4 <- find_lag(tmp4)
lag5 <- find_lag(tmp5)
lag6 <- find_lag(tmp6)

all_lag <- rbind(lag1, lag2, lag3, lag4, lag5, lag6)

mean_lag <- all_lag %>%
  group_by(rank) %>%
  summarise(mean_lag = mean(lag),
            se_lag = sd(lag)/sqrt(n()))

# Make plot
png("./Figures/Flood Figures/GW Mean Lag Time_2020-03-20.png", units = "in", res = 150, width = 7, height = 4)
par(mfrow = c(1,1), mar = c(4,6,1,1), oma = c(0,0,0,0))
plot(mean_lag$rank, mean_lag$mean_lag, pch = 19, bg = "grey", xlab = "", ylab = "",
        xlim = c(0.5, 6.5), ylim = c(0,15), xaxt = "n", cex = 1.5)
with(mean_lag, arrows(rank, mean_lag-se_lag, rank, mean_lag+se_lag, code = 3, angle = 90))
axis(1, at=c(1:6), labels=c('1: HA02','2: HA12','3: HA15','4: HA10','5: HA08','6: HA07'),
     las=1, adj=0)
title(ylab = "Lag (Mean +/- SE) Relative to Main Stem (hr)", line = 2.5)
title(xlab = "Rank Distance from Main Stem", line = 4.5)
dev.off()


##############################################################

# How does the lag time change with a larger chunk of data?

# Here are the sampled periods used above
# tmp1 <- flood_finder("2017-03-08", "2017-03-25")
# tmp2 <- flood_finder("2016-10-06", "2016-10-14")
# tmp3 <- flood_finder("2014-04-01", "2014-04-18")
# tmp4 <- flood_finder("2014-04-25", "2014-05-10")
# tmp5 <- flood_finder("2014-11-26", "2014-12-10")

# Expand these intervals to include two months with the focal flood roughly in the center of that data
tmp1 <- flood_finder("2017-02-01", "2017-04-01")
tmp2 <- flood_finder("2016-09-01", "2016-11-01")
tmp3 <- flood_finder("2014-03-01", "2014-05-01")
tmp4 <- flood_finder("2014-04-01", "2014-06-01")
tmp5 <- flood_finder("2014-11-01", "2015-01-01")

lag1 <- find_lag(tmp1)
lag2 <- find_lag(tmp2)
lag3 <- find_lag(tmp3)
lag4 <- find_lag(tmp4)
lag5 <- find_lag(tmp5)

all_lag <- rbind(lag1, lag2, lag3, lag4, lag5)

mean_lag <- all_lag %>%
  group_by(rank) %>%
  summarise(mean_lag = mean(lag),
            se_lag = sd(lag)/sqrt(n()))

# Make plot
png("./Figures/Flood Figures/GW Mean Lag Time_2month_2020-03-20.png", units = "in", res = 150, width = 7, height = 4)
par(mfrow = c(1,1), mar = c(4,6,1,1), oma = c(0,0,0,0))
plot(mean_lag$rank, mean_lag$mean_lag, pch = 19, bg = "grey", xlab = "", ylab = "",
        xlim = c(0.5, 6.5), ylim = c(0,7), xaxt = "n", cex = 1.5)
with(mean_lag, arrows(rank, mean_lag-se_lag, rank, mean_lag+se_lag, code = 3, angle = 90))
axis(1, at=c(1:6), labels=c('1: HA02','2: HA12','3: HA15','4: HA10','5: HA08','6: HA07'),
     las=1, adj=0)
title(ylab = "Lag (Mean +/- SE) Relative to Main Stem (hr)", line = 2.5)
title(xlab = "Rank Distance from Main Stem", line = 4.5)
dev.off()


########################################################

# What if I expand the window to 1 year?
tmp1 <- flood_finder("2012-01-01", "2012-12-31")
tmp2 <- flood_finder("2013-01-01", "2013-12-31")
tmp3 <- flood_finder("2014-01-01", "2014-12-31")
tmp4 <- flood_finder("2015-01-01", "2015-12-31")
tmp5 <- flood_finder("2016-01-01", "2016-12-31")
tmp6 <- flood_finder("2017-01-01", "2017-12-31")
tmp7 <- flood_finder("2018-01-01", "2018-12-31")
tmp8 <- flood_finder("2019-01-01", "2019-12-31")

lag1 <- find_lag(tmp1)
lag2 <- find_lag(tmp2)
lag3 <- find_lag(tmp3)
lag4 <- find_lag(tmp4)
lag5 <- find_lag(tmp5)
lag6 <- find_lag(tmp6)
lag7 <- find_lag(tmp7)
lag8 <- find_lag(tmp8)

all_lag <- rbind(lag1, lag2, lag3, lag4, lag5)

mean_lag <- all_lag %>%
  group_by(rank) %>%
  summarise(mean_lag = mean(lag),
            se_lag = sd(lag)/sqrt(n()))

# Make plot
png("./Figures/Flood Figures/GW Mean Lag Time_1year_2020-03-20.png", units = "in", res = 150, width = 7, height = 4)
par(mfrow = c(1,1), mar = c(4,6,1,1), oma = c(0,0,0,0))
plot(mean_lag$rank, mean_lag$mean_lag, pch = 19, bg = "grey", xlab = "", ylab = "",
        xlim = c(0.5, 6.5), ylim = c(0,7), xaxt = "n", cex = 1.5)
with(mean_lag, arrows(rank, mean_lag-se_lag, rank, mean_lag+se_lag, code = 3, angle = 90))
axis(1, at=c(1:6), labels=c('1: HA02','2: HA12','3: HA15','4: HA10','5: HA08','6: HA07'),
     las=1, adj=0)
title(ylab = "Lag (Mean +/- SE) Relative to Main Stem (hr)", line = 2.5)
title(xlab = "Rank Distance from Main Stem", line = 4.5)
dev.off()


```

