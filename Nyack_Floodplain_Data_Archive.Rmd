---
title: "Nyack Floodplain Data Archiving"
author: "Amalia Handler"
date: "6/24/2019"
output: 
  html_document: 
    keep_md: yes
editor_options: 
  chunk_output_type: console
---

Code to compile and organize Nyack Floodplain data meterological and hydrologic data into an archivable format.

Approach to start
- Figure out the structure of each df for HA07. Determine what is duplicate data and if anything has changed

Load the data
```{r include = FALSE}
options(stringsAsFactors = FALSE)
library(tidyverse)

# Get the data
temp <- read.delim("./Data/CR1000_HA15_Table1.dat", 
                   sep = ",", skip = 4, header = FALSE)

# Add the column headers
colnames(temp) <- colnames(read.delim("./Data/CR1000_HA15_Table1.dat", 
                                      sep = ",", skip = 1, header = TRUE))

head(temp)

ha07_files <- list.files(path = "./Data/HA07")

ha07_water_files <- ha07_files[grepl("_Water", ha07_files)]

# Testing te function
file_name <- "CR1000_HA07_Water - thru6_7_2016.dat"

# Write a function to load in the water files
load_ha07_water_file <- function(file_name){
  # Read in the file
  temp <- read.delim(paste("./Data/HA07/", file_name, sep = ''), 
                     sep = ",", skip = 4, header = FALSE)
  
  colnames(temp) <- colnames(read.delim(paste("./Data/HA07/", file_name, sep = ''), 
                                        sep = ",", skip = 1, header = TRUE))
  
  # Find the min and max sample dates in the file
  # Convert the timestamp variable to posix time
  temp$TIMESTAMP <- as.POSIXct(temp$TIMESTAMP, format = "%Y-%m-%d %H:%M:%S") 
  
  # Find the max
  max_time <- format(max(na.omit(temp$TIMESTAMP)), "%Y_%m_%d")
  
  # Find the min
  min_time <- format(min(na.omit(temp$TIMESTAMP)), "%Y_%m_%d")
  
  # Create a new file name using the station number, min, and max datetime
  new_name <- paste("HA07", min_time, "to", max_time, sep = '_')
  
  # Write the file to the same location in rds format
  saveRDS(temp, paste("./Data/HA07/", new_name, sep = ''))

}

# Apply the read in function to the water files in the directory
for(file in ha07_water_files){
  load_ha07_water_file(file)
}


ha07_1 <- readRDS(paste("./Data/HA07/", "HA07_2012_05_15_to_2012_07_06", sep = ''))
ha07_2 <- readRDS(paste("./Data/HA07/", "HA07_2012_07_06_to_2012_11_24", sep = ''))
ha07_3 <- readRDS(paste("./Data/HA07/", "HA07_2012_07_06_to_2013_03_31", sep = ''))
ha07_4 <- readRDS(paste("./Data/HA07/", "HA07_2013_04_01_to_2016_06_07", sep = ''))
ha07_5 <- readRDS(paste("./Data/HA07/", "HA07_2013_04_01_to_2018_07_10", sep = ''))
ha07_6 <- readRDS(paste("./Data/HA07/", "HA07_2013_04_01_to_2018_07_16", sep = ''))
ha07_7 <- readRDS(paste("./Data/HA07/", "HA07_2013_04_01_to_2019_06_05", sep = ''))

nrow(HA07_1)
nrow(HA07_2)



# Identify the rows that have NA values in the timestamp column
temp %>%
       rowid_to_column() %>%
     filter(is.na(TIMESTAMP))

```


I need to figure out earliest date for each station and the variables in each dataframe.

```{r include = FALSE}
# List the files in the directory from Phil
ha07_files <- list.files(path = "./Data/HA07")

# Identify only those files related to water
ha07_water_files <- ha07_files[grepl("_Water", ha07_files)]

# Read in the files
toread <- paste("./Data/HA07/", ha07_water_files, sep = '')

ha07_water <- list()
ha07_water <- lapply(toread, read.delim, header = FALSE, sep = ',', skip = 4) 

# Get the column names
var_names <- list()
for(i in 1:length(ha07_water_files)){
  var_names[[i]] <- colnames(read.delim(paste("./Data/HA07/", ha07_water_files[i], sep = ''), sep = ",", skip = 1, header = TRUE))
}

# Add the column names to the list of files
ha07_water <- lapply(ha07_water, function(x) {
  colnames(x) <- var_names[[1]]
  x      
})

# Convert the timestamp to a datetime variable
ha07_water <- lapply(ha07_water, function(x){
 x$TIMESTAMP <- as.POSIXct(x$TIMESTAMP, format = "%Y-%m-%d %H:%M:%S")
 return(x)
})


# Find the max and min data in each file
max_min_date <- lapply(ha07_water, function(x){
  min_date <- format(as.Date(min(na.omit(x$TIMESTAMP))), "%Y_%m_%d")
  max_date <- format(as.Date(max(na.omit(x$TIMESTAMP))), "%Y_%m_%d")
  dates <- c(min_date, max_date)
  return(dates)
})

# Name the dataframes in the list
ha07_water_df_names <- sapply(max_min_date, function(x){
                              paste("ha07_water", x[1], "to", x[2], sep = '_')
})

names(ha07_water) <- ha07_water_df_names

# Earliest date of water data in these files
min_date <- NULL
max_date <- NULL
for(i in 1:length(max_min_date)){
  min_date[i] <- max_min_date[[i]][1]
  max_date[i] <- max_min_date[[i]][2]
}

min_date <- as.Date(min_date, format = "%Y_%m_%d")
max_date <- as.Date(max_date, format = "%Y_%m_%d")

min(min_date)
max(max_date)


```

Now read in the meterological data from HA07

```{r include = FALSE}
# List the files in the directory from Phil
ha07_files <- list.files(path = "./Data/HA07")

# Identify only those files related to water
ha07_met_files <- ha07_files[grepl("_Met", ha07_files)]

# Read in the files
toread <- paste("./Data/HA07/", ha07_met_files, sep = '')

ha07_met <- list()
ha07_met <- lapply(toread, read.delim, header = FALSE, sep = ',', skip = 4) 

# Get the column names
var_names <- list()
for(i in 1:length(ha07_met_files)){
  var_names[[i]] <- colnames(read.delim(paste("./Data/HA07/", ha07_met_files[i], sep = ''), sep = ",", skip = 1, header = TRUE))
}

# Apply the column names to the list of dfs
ha07_met <- lapply(ha07_met, function(x) {
  colnames(x) <- var_names[[1]]
  x      
})

# Convert the timestamp to a datetime variable
ha07_met <- lapply(ha07_met, function(x){
 x$TIMESTAMP <- as.POSIXct(x$TIMESTAMP, format = "%Y-%m-%d %H:%M:%S")
 return(x)
})


# Find the max and min data in each file
max_min_date <- lapply(ha07_met, function(x){
  min_date <- format(as.Date(min(na.omit(x$TIMESTAMP))), "%Y_%m_%d")
  max_date <- format(as.Date(max(na.omit(x$TIMESTAMP))), "%Y_%m_%d")
  dates <- c(min_date, max_date)
  return(dates)
})

# Name the dataframes in the list
ha07_met_df_names <- sapply(max_min_date, function(x){
                              paste("ha07_met", x[1], "to", x[2], sep = '_')
})

names(ha07_met) <- ha07_met_df_names

# Earliest date of water data in these files
min_date <- NULL
max_date <- NULL
for(i in 1:length(max_min_date)){
  min_date[i] <- max_min_date[[i]][1]
  max_date[i] <- max_min_date[[i]][2]
}

min_date <- as.Date(min_date, format = "%Y_%m_%d")
max_date <- as.Date(max_date, format = "%Y_%m_%d")

min(min_date)
max(max_date)

```


Now load in the data from Jeremy

```{r include = FALSE}
# List the files in the directory from Phil
ha_files_toread <- list.files(path = "./Data", pattern = ".dat")

# Read in the files
toread <- paste("./Data/", ha_files_toread, sep = '')

ha_files <- list()
ha_files <- lapply(toread, read.delim, header = FALSE, sep = ',', skip = 4) 

# Get the column names
var_names <- list()
for(i in 1:length(ha_files)){
  var_names[[i]] <- colnames(read.delim(paste("./Data/", ha_files_toread[i], sep = ''), sep = ",", skip = 1, header = TRUE))
}

# Apply the column names to the list of dfs
for(i in 1:length(var_names)){
  colnames(ha_files[[i]]) <- var_names[[i]]
}

# Convert the timestamp to a datetime variable
ha_files <- lapply(ha_files, function(x){
 x$TIMESTAMP <- as.POSIXct(x$TIMESTAMP, format = "%Y-%m-%d %H:%M:%S")
 return(x)
})

# Find the max and min data in each file and determine if it is a water data files or a meteorlogical file
file_info <- lapply(ha_files, function(x){
  if(colnames(x)[3] == "AirTC") {
    data_type <- "met"
  } else { data_type <- "water"
  }
  min_date <- format(as.Date(min(na.omit(x$TIMESTAMP))), "%Y_%m_%d")
  max_date <- format(as.Date(max(na.omit(x$TIMESTAMP))), "%Y_%m_%d")
  dates <- c(data_type, min_date, max_date)
  return(dates)
})

# Find the ha number for each file
# If the file contains "CR1000", then I can extract
# If the file name contains "Nyack" then need another method
well_info <- NULL
for(i in 1:length(ha_files_toread)){
  if(grepl("CR1000", ha_files_toread[i])){
  well_info[i] <- substr(ha_files_toread[i], 8, 11)
  } else {
    well_info[i] <- substr(ha_files_toread[i], 7, 10)
  }
}

# Name the dataframes in the list
ha_df_names <- sapply(max_min_date, function(x){
                              paste("ha07_met", x[1], "to", x[2], sep = '_')
  })

ha_df_names <- NULL
for(i in 1:length(well_info)){
  ha_df_names[i] <- paste(well_info[i], file_info[[i]][1], file_info[[i]][2], "to", file_info[[i]][3], sep = "_")
}

names(ha_files)  <- ha_df_names
names(var_names) <- ha_df_names

# Need to coerce the mutiple columns in the HA02 df from character to number. Seems that R does not see "NAN" is equivelent to "NaN".
column_numbers <- c(11, 14:17)
for(col in column_numbers){
  ha_files$HA02_water_2013_04_01_to_2019_06_23[,col] <- 
    as.numeric(ha_files$HA02_water_2013_04_01_to_2019_06_23[,col])
}

```


Jeremy sent a whole set of data that include data prior to 2013-04-01
Going through the metadata for each station, I found the start date for each station. This is the date at which it was confirmed that the station is collecting reliable data.

Station, start date
CASC, 2012-09-06
HA02, 2012-06-08
HA07, 2012-05-15 (met station and well)
HA08, 2011-12-14 (well)
HA08, 2012-05-02 (Beaver Creek)
HA10, 2011-12-16
HA12, 2012-04-26
HA15, 2012-04-26

# Append this older data to the longer contemporary datasets that Jeremy sent

```{r}

# Find all the initial data files with data between the start of monitoring and the start of the data from Jeremy (2013-04-01)
ini_files <- c(
  './Data/Nyack_RiverNET_HA02 Movie Rd/Nyack HA02 MovRd_Table1 5_8_12 thru 3_31_13.dat',
  './Data/Nyack_RiverNET_HA07 MET/CR1000_HA07_Met_5_15_12 thru 7_6_12.dat',
  './Data/Nyack_RiverNET_HA07 MET/CR1000_HA07_Met 7_6_12 thru 3_31_13.dat',
  './Data/Nyack_RiverNET_HA07 MET/CR1000_HA07_Water_5_15_12 thru 7_6_12.dat',
  './Data/Nyack_RiverNET_HA07 MET/CR1000_HA07_Water 7_6_12 thru 3_31_13.dat',
  './Data/Nyack_RiverNET_HA08_Cabin/Manual download/Nyack HA08 Cabin_Table1 5_2_12 well only.dat',
  './Data/Nyack_RiverNET_HA08_Cabin/Manual download/Nyack HA08 Cabin_Table1 5_2_12 thru 11_28_12.dat',
  './Data/Nyack_RiverNET_HA08_Cabin/CR1000_HA08 Cabin_Table1 8_23_12 thru 3_31_13.dat',
  './Data/Nyack_RiverNET_HA10_Sarg/CR1000_HA10_Table1 12_16_11 thru 3_31_13.dat',
  './Data/Nyack_RiverNET_HA12_Methane/CR1000_HA12_Table1 4_26_12 thru 3_31_13.dat',
  './Data/Nyack_RiverNET_HA15_Springhead/CR1000_HA15_Table1 4_26_12 thru 3_31_13.dat',
  './Data/Nyack_RiverNET_Cascadilla/Nyack CASC_Table1 9_6_12 thru 3_31_13.dat')

ini_ha_files <- lapply(ini_files, read.delim, header = FALSE, sep = ',', skip = 4) 

# Get the column names
var_names <- list()
for(i in 1:length(ini_ha_files)){
  var_names[[i]] <- colnames(read.delim(ini_files[i], sep = ",", skip = 1, header = TRUE))
}

# Apply the column names to the list of dfs
for(i in 1:length(var_names)){
  colnames(ini_ha_files[[i]]) <- var_names[[i]]
}

# Convert the timestamp to a datetime variable
ini_ha_files <- lapply(ini_ha_files, function(x){
 x$TIMESTAMP <- as.POSIXct(x$TIMESTAMP, format = "%Y-%m-%d %H:%M:%S")
 return(x)
})

# Apply names to the dataframes
names(ini_ha_files) <- c('ha02', 'ha07met', 'ha07met', 'ha07wtr', 'ha07wtr', 'ha08', 'ha08', 'ha08', 'ha10', 'ha12', 'ha15', 'casc')

# Coerce one of the column from one of the HA08 files from character to numeric
ini_ha_files[[6]]$DoConc2 <- as.numeric(ini_ha_files[[6]]$DoConc2)

# Bind together rows from the same station
ini_ha07met <- rbind(ini_ha_files[[2]], ini_ha_files[[3]])
ini_ha07wtr <- rbind(ini_ha_files[[4]], ini_ha_files[[5]])
ini_ha08    <- rbind(ini_ha_files[[6]], ini_ha_files[[7]], ini_ha_files[[8]])

# Remove duplicates from the above bound files
ini_ha07met <- ini_ha07met[!duplicated(ini_ha07met),]
ini_ha07wrt <- ini_ha07wtr[!duplicated(ini_ha07wtr),]
ini_ha08    <- ini_ha08[!duplicated(ini_ha08),]

# Make a new list, now with the bound, duplicates removed data
ini_ha_files <- list(ini_ha_files$casc, ini_ha_files$ha02, ini_ha07met, ini_ha07wtr, ini_ha08, ini_ha_files$ha10, ini_ha_files$ha12, ini_ha_files$ha15)

# Bind the initial data to the data from Jeremy

# First order the data
ha_files <- ha_files[order(names(ha_files))]

# Now bind together
ha_comb <- list()
for(i in 1:length(ha_files)){
  ha_comb[[i]] <- rbind(ini_ha_files[[i]], ha_files[[i]])
}

# Name the dfs
names(ha_comb) <- c('casc', 'ha02', 'ha07met', 'ha07wtr', 'ha08', 'ha10', 'ha12', 'ha15')

# Find the max and min data in each file and determine if it is a water data files or a meteorlogical file
file_info <- lapply(ha_comb, function(x){
  if(colnames(x)[3] == "AirTC") {
    data_type <- "met"
  } else { data_type <- "water"
  }
  min_date <- format(as.Date(min(na.omit(x$TIMESTAMP))), "%Y_%m_%d")
  max_date <- format(as.Date(max(na.omit(x$TIMESTAMP))), "%Y_%m_%d")
  dates <- c(data_type, min_date, max_date)
  return(dates)
})

# Renames the timestamp column and remove the records column in the met data
# Add a site column
ha07_met <- ha_comb$ha07met
ha07_met$site <- "ha07" 
ha07_met$sensor_number <- "met"
colnames(ha07_met)[1] <- "datetime"
ha07_met <- ha07_met[, c(21,22,1,3:20)]

# Save the meterological data file
# saveRDS(ha07_met, './Data/HA_Met_Data.rds')
# write.csv(ha07_met, './Data/HA_Met_Data.csv', row.names = FALSE)

```


Compile all water data into one df
Then separate into three spreadsheets for DO, Conductivity, and Water Level


```{r}

# For HA02, 08, and 15, there are two sets of sensors. Need to separate these into different dataframes
# HA02 has sensors 9 and 10
# HA08 has sensors 1 and 2
# HA15 has sensors 7 and 8
ha02_09 <- ha_comb$ha02[,colnames(ha_comb$ha02)[1:10]]
ha02_10 <- ha_comb$ha02[,colnames(ha_comb$ha02)[c(1,2,11:18)]]
ha08_01 <- ha_comb$ha08[,colnames(ha_comb$ha08)[1:10]]
ha08_02 <- ha_comb$ha08[,colnames(ha_comb$ha08)[c(1,2,11:18)]]
ha15_07 <- ha_comb$ha15[,colnames(ha_comb$ha15)[1:10]]
ha15_08 <- ha_comb$ha15[,colnames(ha_comb$ha15)[c(1,2,11:18)]]

# HA08_02 didn't come online until 2012-05-02. Need to trim this df to only include dates greater than and including this date. From looking at the dataframe, it looks like the first numbers appear at 11:00:00.

# TO avoid headaches with the rows that are missing datetime information, and because the df is organized in chronological order, I'll use a match function rather than a logical (>) to select the observations that come after 2012-05-02.
first_row <- match(as.POSIXct('2012-05-02 11:00:00'), ha08_02$TIMESTAMP)
ha08_02   <- ha08_02[first_row:nrow(ha08_02),] 

# Check the columns that have NA values for time
ha08_02[is.na(ha08_02$TIMESTAMP),]

# HA02_10 was initially installed in a parafluvial area, but quickly went dry and was removed from the area. It was removed on 2012-09-21 around 15:00. Note that the sensor was dry for some time before it was removed (first noted as dry on 2012-07-12)
# Looking at the data, the water level switches from positive to negative at 2012-07-11 11:00:00
# Remove data for this site after this point in time
last_row <- match(as.POSIXct('2012-07-11 11:00:00'), ha02_10$TIMESTAMP)
ha02_10  <- ha02_10[1:last_row,]

# Recombine into a list
ha_sep <- list(ha_comb$casc, ha02_09, ha02_10, ha_comb$ha07wtr,
               ha08_01, ha08_02, ha_comb$ha10, ha_comb$ha12,
               ha15_07, ha15_08)

# Set the site and sensor numbers
site <- c('CASC', 'HA02', 'HA02', 'HA07', 'HA08', 'HA08', 'HA10', 'HA12', 'HA15', 'HA15')

sensor <- sapply(ha_sep, function(x){
  col_name <- colnames(x)[10]
  if(nchar(col_name) == 8){
    num <- substr(col_name, 8, 8)
    sensor_number <- paste('0', num, sep = '')
  } else {
    sensor_number <- substr(col_name, 8, 9)
  }
  return(sensor_number)
})


# Add columns to each df for the site and the sensor number
for(i in 1:length(ha_sep)){
  ha_sep[[i]]$site <- site[i]
  ha_sep[[i]]$sensor_number <- sensor[i]
}

# Now bind together all the files
# Need to rename all the columns first for compatabiility
# New column names
col_names <- c('datetime', 'record', 'do_conc', 'do_sat', 'do_temp', 'cond', 'ct', 'cond_temp', 'level_m', 'level_temp', 'site', 'sensor_number')

# Rename the columns for the dfs in the list
ha_rename <- lapply(ha_sep, function(x){
  colnames(x) <- col_names
  return(x)
})

# Bind together all rows
ha_all <- do.call("rbind", ha_rename)

# Check that the number of rows in ha_all is the sum of the rows from ha_rename
sum(sapply(ha_rename, nrow)) == nrow(ha_all)
# Perfecto!

# Rearrange the columns
ha_water <- ha_all[, c(11,12,1,3:10)]

# Save the new master file
# saveRDS(ha_water, './Data/All_HA_Water_Data.rds')
# write.csv(ha_water, './Data/All_HA_Water_Data.csv', col.names = TRUE, row.names = FALSE)

```

What kind of NA values are in the df?

```{r}
# ha_water <- readRDS('./Data/All_HA_Water_Data')

# What timea are missing
temp <- subset(ha_water, is.na(ha_water$datetime))

# Check for missing datetime information
# Datetime was always missing around March 10, always at 02:00:00, must be due to daylight savings time. In spring, clocks go from 2 to 3 am instantly, thus losing an hour. Yet, there is still an observation collected between 1 and 3 am on these dates. Why?
ha_water[(which(ha_water$site == "CASC" & is.na(ha_water$datetime)) - 1),]
ha_water[(which(ha_water$site == "CASC" & is.na(ha_water$datetime)) + 1),]

ha_water[(which(ha_water$site == "HA02" & is.na(ha_water$datetime)) - 1),]
ha_water[(which(ha_water$site == "HA02" & is.na(ha_water$datetime)) + 1),]

ha_water[(which(ha_water$site == "HA07" & is.na(ha_water$datetime)) - 1),]
ha_water[(which(ha_water$site == "HA07" & is.na(ha_water$datetime)) + 1),]

ha_water[(which(ha_water$site == "HA08" & is.na(ha_water$datetime)) - 1),]
ha_water[(which(ha_water$site == "HA08" & is.na(ha_water$datetime)) + 1),]

ha_water[(which(ha_water$site == "HA10" & is.na(ha_water$datetime)) - 1),]
ha_water[(which(ha_water$site == "HA10" & is.na(ha_water$datetime)) + 1),]

ha_water[(which(ha_water$site == "HA12" & is.na(ha_water$datetime)) - 1),]
ha_water[(which(ha_water$site == "HA12" & is.na(ha_water$datetime)) + 1),]

ha_water[(which(ha_water$site == "HA15" & is.na(ha_water$datetime)) - 1),]
ha_water[(which(ha_water$site == "HA15" & is.na(ha_water$datetime)) + 1),]


# Identifying missing data based on when the sensors were serviced

# As an example, sensors were out at HA02_09 at 11:00 on 2012-06-26
time_out <- as.POSIXct('2012-06-26 11:00', format = '%Y-%m-%d %H:%M')
site     <- "HA02"
sensor   <- "09"

temp <- ha_water[ha_water$site == site & ha_water$sensor_number == sensor,]

row_num <- match(time_out, temp$datetime)

temp[(row_num - 3):(row_num + 3), 3:11]

# As an example, sensors were out at HA02_09 at 11:00 on 2012-06-26
time_out <- as.POSIXct('2012-07-23 19:00', format = '%Y-%m-%d %H:%M')
site     <- "HA02"
sensor   <- "09"

temp <- ha_water[ha_water$site == site & ha_water$sensor_number == sensor,]

row_num <- match(time_out, temp$datetime)

temp[(row_num - 3):(row_num + 3), 3:11]

# Another example
time_out <- as.POSIXct('2012-07-26 12:00', format = '%Y-%m-%d %H:%M')
site     <- "HA10"
sensor   <- "04"

temp <- ha_water[ha_water$site == site & ha_water$sensor_number == sensor,]

row_num <- match(time_out, temp$datetime)

temp[(row_num - 3):(row_num + 3), 3:11]

# Another example
time_out <- as.POSIXct('2014-10-07 14:00', format = '%Y-%m-%d %H:%M')
site     <- "HA15"
sensor   <- "07"

temp <- ha_water[ha_water$site == site & ha_water$sensor_number == sensor,]

row_num <- match(time_out, temp$datetime)

temp[(row_num - 3):(row_num + 3), 3:11]

# Example from a thing in surface water
time_out <- as.POSIXct('2014-05-08 11:00', format = '%Y-%m-%d %H:%M')
site     <- "CASC"
sensor   <- "06"

temp <- ha_water[ha_water$site == site & ha_water$sensor_number == sensor,]

row_num <- match(time_out, temp$datetime)

temp[(row_num - 3):(row_num + 3), 3:11]

# In general, when the sensors are being serviced, the water level is negative, the ct is negative, and the conductivity is below 0.005 mS (below 0.010)

# Let's see if I can pull out this data
bad_data <- ha_water[which(ha_water$cond < 0.010 & ha_water$ct < 0 & ha_water$level_m < 0),]
# Seems like this is flagging data from service trips where the sensors were pulled for a measurement.
# All sites now have approximately the same number of flagged observations
# From comparing the metadata to the datetimes flagged in this df, there are some observations that were flagged at times when there is no note in the metadata about removing sensors. This seems fairly reasonable. If someone was working in the well, but wasn't out with Tom or Phil, then it probably didn't make it into the metadata notes.
# Note also that this processes misses observations that satisfy one or two of these qualifiers, but have NA or NaN values for one or two of the others.
nrow(bad_data)

bad_do <- (ha_water[ha_water$do_conc <= 0 | is.na(ha_water$do_conc),])

# First thing that jumps out is that the ice up on CASC meant that the sensors could not be cleaned or calibrated and the DO readings for this period are all zero. Ice cover first recorded 11/27/2013, pouring ice melt down the well enabled calibration on 12/27/2013, then were frozen again until 4/3/2014

temp <- bad_do[which(bad_do$site == "CASC" & bad_do$do_conc == 0),]
unique(as.Date(temp$datetime))



```

Try out some different anomomly detection methods

```{r}

# ha_water <- readRDS("./Data/All_HA_Water_Data.rds")

# Anomalize R Package
# Useful articles with some background on anomaly detection

# This describes the methods for generating seasonality and trend data
# https://cran.r-project.org/web/packages/anomalize/vignettes/anomalize_methods.html

# This describes how to alter the amount of anomalies detected
# https://cran.r-project.org/web/packages/anomalize/vignettes/anomalize_quick_start_guide.html#adjusting-decomposition-trend-and-seasonality

# devtools::install_github("business-science/anomalize")

library(tidyverse)
library(anomalize)

# Test out the anomoly detection for a small subset of data
do_temp <- ha_water[ha_water$site == 'HA07' & ha_water$sensor_number == '03',]

# DO temperature from 01-2013 to 06-2013 for HA07
do_temp <- ha_water[ha_water$site == 'HA07' & ha_water$sensor_number == '03' & ha_water$datetime >= as.POSIXct('2013-01-01 00:00') & ha_water$datetime <= as.POSIXct('2013-06-01 00:00'), c('do_temp','datetime')]

do_temp <- as_tibble(do_temp)
do_temp <- do_temp[complete.cases(do_temp),]
do_temp <- as_tbl_time(do_temp, datetime)

do_temp %>%
    # Data Manipulation / Anomaly Detection
    time_decompose(do_temp, method = "stl") %>%
    anomalize(remainder, method = "iqr") %>%
    time_recompose() %>%
    # Anomaly Visualization
    plot_anomalies(time_recomposed = TRUE, ncol = 3, alpha_dots = 0.25) +
    scale_y_continuous(limits = c(4, 7)) +
    labs(title = "HA07 DO Temp Anomalies", subtitle = "STL + IQR Methods") 

# With defaults for freq and time
decomp_do_temp <- do_temp %>%
    time_decompose(do_temp, method = "stl") %>%
    anomalize(remainder, method = "iqr") %>%
    time_recompose()
 
p1 <- decomp_do_temp %>%
    plot_anomaly_decomposition() +
    ggtitle("Freq/Trend = 'auto'")

p1

# How many outliers IDed?
decomp_do_temp[decomp_do_temp$anomaly == "Yes",]
# 131

# Now experiment with setting the freq
anom_do_temp <- do_temp[1:600,] %>%
    time_decompose(do_temp, method = "stl",
                   frequency = "auto",
                   trend = "auto") %>%
    anomalize(remainder, method = "iqr") %>%
    time_recompose() %>%
    plot_anomaly_decomposition()

# How many outliers IDed?
anom_do_temp$data[anom_do_temp$data$anomaly == "Yes",]
# 7


# Try using the outlier detection for a year of data
do_temp <- ha_water[ha_water$site == 'HA07' & ha_water$sensor_number == '03' & ha_water$datetime >= as.POSIXct('2013-01-01 00:00') & ha_water$datetime < as.POSIXct('2014-01-01 00:00'), c('do_temp','datetime')]

do_temp <- as_tibble(do_temp)
do_temp <- do_temp[complete.cases(do_temp),]

anom_do_temp <- do_temp %>%
    time_decompose(do_temp, method = "stl",
                   frequency = "auto",
                   trend = "auto") %>%
    anomalize(remainder, method = "iqr") %>%
    time_recompose()

plot_anomaly_decomposition(anom_do_temp)

anom_do_temp[anom_do_temp$anomaly == "Yes",]

anom_do_temp %>%
plot_anomalies(time_recomposed = TRUE)

anom_do_temp %>%
plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = c(4.7,4.85)) +
  scale_x_datetime(limits = c(as.POSIXct('2013-05-01'),as.POSIXct('2013-05-17')))


# Try using the outlier detection for all the data for HA07 well
do_temp <- ha_water[ha_water$site == 'HA07' & ha_water$sensor_number == '03', c('do_temp','datetime')]

do_temp <- as_tibble(do_temp)
do_temp <- do_temp[complete.cases(do_temp),]

anom_do_temp <- do_temp %>%
    time_decompose(do_temp, method = "stl",
                   frequency = "auto",
                   trend = "auto") %>%
    anomalize(remainder, method = "iqr") %>%
    time_recompose()

plot_anomaly_decomposition(anom_do_temp)

anom_do_temp[anom_do_temp$anomaly == "Yes",]

anom_do_temp %>%
plot_anomalies(time_recomposed = TRUE)

# Seems like there are a lot of anomalies from 2017-2019
nrow(anom_do_temp[anom_do_temp$anomaly == "Yes",])
nrow(anom_do_temp[anom_do_temp$anomaly == "Yes" & anom_do_temp$datetime > as.POSIXct('2017-01-01'),])
# Yup, 88% of outliers fall in thos last two years

# Take a close look at a big cluster of flagged outliers
anom_do_temp %>%
plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = c(6.5,7.75)) +
  scale_x_datetime(limits = c(as.POSIXct('2017-11-01'),as.POSIXct('2018-01-01')))
# Unclear why the trend deviates from the observations so much during this period
# Maybe it's related to the outlier observations on the low end here??

# Would it help if I removed impossible observations?
# for this data, anything outside of 2 and 10 degrees C
do_temp <- do_temp[do_temp$do_temp > 2 & do_temp$do_temp < 10,]

anom_do_temp <- do_temp %>%
    time_decompose(do_temp, method = "stl",
                   frequency = "auto",
                   trend = "auto") %>%
    anomalize(remainder, method = "iqr") %>%
    time_recompose()

anom_do_temp[anom_do_temp$anomaly == "Yes",]

anom_do_temp %>%
plot_anomalies(time_recomposed = TRUE)
# NOPE, did not help, abort

# Maybe try divying up the data into year-long segments?
# That does not fix it

anom_do_temp %>%
plot_anomalies(time_recomposed = TRUE) +
  scale_x_datetime(limits = as.POSIXct(c('2017-10-01 00:00', '2018-10-01 23:00')))

anom_do_temp <- do_temp[do_temp$datetime > as.POSIXct('2017-10-01 00:00') & do_temp$datetime < as.POSIXct('2018-10-01 23:00'),] %>%
    time_decompose(do_temp, method = "stl",
                   frequency = "auto",
                   trend = "auto") %>%
    anomalize(remainder, method = "iqr") %>%
    time_recompose()

anom_do_temp %>%
  plot_anomalies(time_recomposed = TRUE)

decomp_plot <- plot_anomaly_decomposition(anom_do_temp)

nrow(anom_do_temp[anom_do_temp$anomaly == "Yes",])

# Changing frequency to 2 weeks reduced the number of outliers by ~1/3, but it's still identifing a lot of fine data...

anom_do_temp %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_x_datetime(limits = as.POSIXct(c('2017-10-01 00:00', '2017-12-15 23:00'))) +
  scale_y_continuous(limits = c(6.8,7.75))

ggplot(do_temp, aes(x=datetime, y=do_temp)) + 
  geom_point() +
  scale_x_datetime(limits = as.POSIXct(c('2017-10-01 00:00', '2017-12-15 23:00'))) +
  scale_y_continuous(limits = c(6.8,7.75))

# I think the issue here may be the missing data
# The smoothing function is inexplicably far away from the actual data... No idea why that's occuring and adjusting the trend and frequency doesn't appear to be helping.

# Maybe try the twitter method, though I think this is more relevant to when the data variance changes over time, not missing data

anom_do_temp <- do_temp[do_temp$datetime > as.POSIXct('2017-10-01 00:00') & do_temp$datetime < as.POSIXct('2018-10-01 23:00'),] %>%
    time_decompose(do_temp, method = "twitter",
                   frequency = "auto",
                   trend = "auto") %>%
    anomalize(remainder, method = "iqr") %>%
    time_recompose()

anom_do_temp %>%
  plot_anomalies(time_recomposed = TRUE)
# But this is missing a lot of the outliers

anom_do_temp %>%
  plot_anomalies(time_recomposed = TRUE) + 
  scale_x_datetime(limits = as.POSIXct(c('2017-11-01 00:00', '2017-12-05 23:00'))) +
  scale_y_continuous(limits = c(6,8))

# The twitter method performs better over the periods with missing data, but it misses a lot of outliers becuase of horizontal detection evenlopes.


# Try looking at the data for the periods that it's missing. Are there zeros here or NA's? If I change the data structure, does that improve the outlier detection? What if the seasons data is removed?

do_temp[do_temp$datetime > as.POSIXct('2017-11-23 00:00') & do_temp$datetime < as.POSIXct('2017-11-25 23:50'),]

# The missing data is not even in the df (just missing, not NA or zero values)

# Try doing everything up to the missing data
anom_do_temp <- do_temp[do_temp$datetime < as.POSIXct('2017-01-01 00:00'),] %>%
    time_decompose(do_temp, method = "stl",
                   frequency = "auto",
                   trend = "auto") %>%
    anomalize(remainder, method = "iqr") %>%
    time_recompose()

anom_do_temp %>%
  plot_anomalies(time_recomposed = TRUE)

# Check areas where many observations have been flagged
anom_do_temp %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_x_datetime(limits = as.POSIXct(c('2015-03-15 00:00', '2015-03-23 23:00'))) +
  scale_y_continuous(limits = c(5.2,5.4))
# Between 2017-03-16 and 2017-03-20 there's a bunch of data that was IDed as anomalous
# Maybe look at the water level to see it there was some big change for these dates across variables.

temp <- ha_water[which(ha_water$site == 'HA07' & ha_water$datetime > as.POSIXct('2015-03-15 00:00') & ha_water$datetime < as.POSIXct('2015-03-22 23:00')),]

plot(temp$datetime, temp$do_temp)
plot(temp$datetime, temp$cond_temp, ylim = c(5.1,5.4))
plot(temp$datetime, temp$level_temp)
plot(temp$datetime, temp$level_m)

# Appears that there was a big increase in water level (maybe a flood) that increased the temperature across the probes
# I think this is a call to using a correlated regression identification,  because this jump is present in the other temperature and water level data from this station/

nrow(anom_do_temp[anom_do_temp$anomaly == "Yes" & anom_do_temp$datetime > as.POSIXct('2015-03-15 00:00') & anom_do_temp$datetime < as.POSIXct('2015-03-22 23:00'),])

87/230 * 100
# Almost 40% of the outliers are coming from this section of data

# Can I prevent the analysis from identifying these as outliers by lowering the proportion of the data that can be an outlier?
anom_do_temp_a025 <- do_temp[do_temp$datetime < as.POSIXct('2017-01-01 00:00'),] %>%
    time_decompose(do_temp, method = "stl",
                   frequency = "auto",
                   trend = "auto") %>%
    anomalize(remainder, method = "iqr", alpha = 0.025) %>%
    time_recompose()

anom_do_temp_a050 <- do_temp[do_temp$datetime < as.POSIXct('2017-01-01 00:00'),] %>%
    time_decompose(do_temp, method = "stl",
                   frequency = "auto",
                   trend = "auto") %>%
    anomalize(remainder, method = "iqr", alpha = 0.05) %>%
    time_recompose()

anom_do_temp_a025 %>%
  plot_anomalies(time_recomposed = TRUE) +
  ggtitle("alpha = 0.025")

anom_do_temp_a025 %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_x_datetime(limits = as.POSIXct(c('2015-03-15 00:00', '2015-03-23 23:00'))) +
  scale_y_continuous(limits = c(5.2,5.4))

anom_do_temp_a050 %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_x_datetime(limits = as.POSIXct(c('2012-11-11 00:00', '2012-12-01 00:00'))) +
  scale_y_continuous(limits = c(6.65,6.85))

anom_do_temp_a025 %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_x_datetime(limits = as.POSIXct(c('2012-11-11 00:00', '2012-12-01 00:00'))) +
  scale_y_continuous(limits = c(6.65,6.85))

# Changing alpha to 0.025 seems to have removed erronerously identified outlier data around missing data and jumps in data due to water level changes

temp <- ha_water[which(ha_water$site == 'HA07' & ha_water$datetime > as.POSIXct('2012-11-11 00:00') & ha_water$datetime < as.POSIXct('2012-12-01 00:00')),]

plot(temp$datetime, temp$do_temp)
plot(temp$datetime, temp$cond_temp)
plot(temp$datetime, temp$level_temp)
plot(temp$datetime, temp$level_m)


# Run the whole dataset with alpha 0.025
anom_do_temp_025 <- do_temp %>%
    time_decompose(do_temp, method = "stl",
                   frequency = "auto",
                   trend = "auto") %>%
    anomalize(remainder, method = "iqr", alpha = 0.025) %>%
    time_recompose()

anom_do_temp_025 %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = c(4.5,8)) +
  ggtitle("alpha = 0.025")

anom_do_temp_050 <- do_temp %>%
    time_decompose(do_temp, method = "stl",
                   frequency = "auto",
                   trend = "auto") %>%
    anomalize(remainder, method = "iqr", alpha = 0.05) %>%
    time_recompose()

anom_do_temp_050 %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = c(4.5,8)) +
  ggtitle("alpha = 0.05")

# Using alpha = 0.025 doesn't eliminate the problems with the missing data at the end of 2017
anom_do_temp_025 %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = c(6.5,7.75)) +
  scale_x_datetime(limits = as.POSIXct(c('2017-11-01 00:00', '2017-12-31 23:00'))) +
  ggtitle("alpha = 0.025")

# Try identifying the first missed period of observation and run the process for the full period of observation up to this point. Then I may have to run the chunks without missing data data.

# Look at each year of data
anom_do_temp_025 %>%
  plot_anomalies(time_recomposed = TRUE) +
  # scale_y_continuous(limits = c(6.5,7.75)) +
  scale_x_datetime(limits = as.POSIXct(c('2017-01-01 00:00', '2017-12-31 23:00'))) +
  ggtitle("alpha = 0.025")

anom_do_temp_025 %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = c(6.1,6.35)) +
  scale_x_datetime(limits = as.POSIXct(c('2019-01-01 00:00', '2019-01-14 23:00'))) +
  ggtitle("alpha = 0.025")

# 2019 has some missing data between 01-01 and 01-10
# Try running 2019 as a separate analysis
anom_do_temp_2019 <- do_temp[do_temp$datetime >= as.POSIXct('2019-01-01 00:00'),] %>%
    time_decompose(do_temp, method = "stl",
                   frequency = "auto",
                   trend = "auto") %>%
    anomalize(remainder, method = "iqr", alpha = 0.025) %>%
    time_recompose()

anom_do_temp_2019 %>%
  plot_anomalies(time_recomposed = TRUE) +
  ggtitle("alpha = 0.025")

anom_do_temp_2019 %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = c(6.0,6.35)) +
  scale_x_datetime(limits = as.POSIXct(c('2019-01-01 00:00', '2019-01-14 23:00'))) +
  ggtitle("alpha = 0.025")

# I really think I need a package that will interpolate missing data so that the observations near the missing data won't always get flagged as outliers. It seems like the anomalize package is treating missing data as zeros or something.

```

Figure out how to fill gaps in the data prior to using the anomalize package

```{r}
# install.packages("forecast")
library(forecast)
library(tibbletime)

all_times <- seq(from = min(do_temp$datetime), to = max(do_temp$datetime), by = "1 hour")

temp <- data.frame(datetime = all_times)

temp <- merge(temp, do_temp, by = "datetime", all.x = TRUE)

do_temp <- as_tbl_time(temp, datetime)

temp <- na.interp(do_temp$do_temp)


plot(all_times, temp)

xlims <- as.POSIXct(c('2017-01-01 00:00', '2017-12-31 23:00'))
plot(do_temp$datetime, do_temp$do_temp, xlim = xlims, pch = '.', ylim = c(5,8))
plot(all_times, temp, xlim = xlims, pch = '.', ylim = c(5,8))

do_temp$interp <- temp 

do_temp$interp_val[is.na(do_temp$do_temp)] <- 'yes'
do_temp$interp_val[is.na(do_temp$do_temp) == FALSE] <- 'no'

plot(do_temp$datetime, do_temp$do_temp, xlim = xlims, pch = '.', ylim = c(5,8))
points(do_temp$datetime[do_temp$interp_val == 'yes'], do_temp$interp[do_temp$interp_val == 'yes'], pch = '.', cex = 1.5, col = 'red')

# Look at the area with alot of missing data
xlims <- as.POSIXct(c('2017-11-08 00:00', '2017-11-20 23:00'))
ylims <- c(7.2, 7.5)

plot(do_temp$datetime, do_temp$do_temp, xlim = xlims, pch = '.', ylim = ylims)

points(do_temp$datetime[do_temp$interp_val == 'yes'], do_temp$interp[do_temp$interp_val == 'yes'], pch = '.', col = 'red')

# This is doing a linear interpolation between points. How do I get it to do a stl interpolation?

# install.packages("imputeTS")
library(imputeTS)

temp <- na_interpolation(do_temp$do_temp, option = "stine")

do_temp$interp <- temp 

do_temp$interp_val[is.na(do_temp$do_temp)] <- 'yes'
do_temp$interp_val[is.na(do_temp$do_temp) == FALSE] <- 'no'

# Look at the area with alot of missing data
xlims <- as.POSIXct(c('2017-11-20 00:00', '2017-11-30 23:00'))
ylims <- c(7.0, 7.4)

plot(do_temp$datetime, do_temp$do_temp, xlim = xlims, pch = '.', ylim = ylims)

points(do_temp$datetime[do_temp$interp_val == 'yes'], do_temp$interp[do_temp$interp_val == 'yes'], pch = '.', col = 'red')


# Now try the anomalize package with the linear and stine interpolation data
# Linear interpolation
temp <- na_interpolation(do_temp$do_temp, option = "linear")

do_temp$interp_lin <- temp 

do_temp$interp_val[is.na(do_temp$do_temp)] <- 'yes'
do_temp$interp_val[is.na(do_temp$do_temp) == FALSE] <- 'no'

anom_do_temp_lin <- do_temp %>%
    time_decompose(interp_lin, method = "stl",
                   frequency = "auto",
                   trend = "2 weeks") %>%
    anomalize(remainder, method = "iqr", alpha = 0.025) %>%
    time_recompose()

anom_do_temp_lin %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = c(4,8))

# Zoom in on interpolated data
xlims <- as.POSIXct(c('2017-11-20 00:00', '2017-11-30 23:00'))
ylims <- c(7.0, 7.4)
anom_do_temp_lin %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = ylims) +
  scale_x_datetime(limits = xlims)

# Zoom in on 2019 data
xlims <- as.POSIXct(c('2019-03-15 00:00', '2019-06-15 23:00'))
ylims <- c(4.6, 5.5)
anom_do_temp_stn %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = ylims) +
  scale_x_datetime(limits = xlims)

# Stine interpolation
temp <- na_interpolation(do_temp$do_temp, option = "stine")

do_temp$interp_stn <- temp 

do_temp$interp_val[is.na(do_temp$do_temp)] <- 'yes'
do_temp$interp_val[is.na(do_temp$do_temp) == FALSE] <- 'no'

anom_do_temp_stn <- do_temp %>%
    time_decompose(interp_stn, method = "stl",
                   frequency = "auto",
                   trend = "2 weeks") %>%
    anomalize(remainder, method = "iqr", alpha = 0.025) %>%
    time_recompose()

anom_do_temp_stn %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = c(4,8))

# Zoom in on interpolated data
xlims <- as.POSIXct(c('2017-11-20 00:00', '2017-11-30 23:00'))
ylims <- c(7.0, 7.4)
anom_do_temp_stn %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = ylims) +
  scale_x_datetime(limits = xlims)

# Look about the same using the linear versus the stine interpolation method
# YAY?! (maybe)

# Look at each year
xlims <- as.POSIXct(c('2019-01-01 00:00', '2019-12-31 23:00'))
ylims <- c(4.5, 7.75)
anom_do_temp_stn %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = ylims) +
  scale_x_datetime(limits = xlims)

xlims <- as.POSIXct(c('2019-03-15 00:00', '2019-06-15 23:00'))
ylims <- c(4.6, 5.5)
anom_do_temp_stn %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = ylims) +
  scale_x_datetime(limits = xlims)


# Okay, I think I found it. Do a linear interpolation, perform anomoly detection using the stl method with the frequency set to 24 hrs and the trend set to two weeks. Then remove outliers using the iqr method with alpha set to 0.025 (6X the inner quartile range). Then merge the outlier information back into the dataframe. 

```


Okay, try and impliment so kind of automated outlier detection

```{r}
library(tidyverse)
library(imputTS)
library(anomalize)
library(tibbletime)

# Anomoly detection, removal, and flagging

# Remove instances where all three sensors are questionable. These are overwhelming instances when the sensors were pulled for servicing/calibration or conducting experiments or collecting samples from the sensor site.

# When the sensors are removed from the water, the data always looks the same, the conductivity is 0.005, the uncorrected conductivity is negative, and the water level is less than zero.

# The dates identified below generally match the service log for the sensors. 
ha_water <- ha_water[-(which(ha_water$cond == 0.005 & 
                             ha_water$ct < 0 & 
                             ha_water$level_m < 0)),]

# Remove impossible numbers

# DO concentration cannot be negative
# Over 8% of DO concentration observations are negative?!
nrow(ha_water[which(ha_water$do_conc < 0), ])

# Bad conductivity data
nrow(ha_water[which(ha_water$ct < 0), ])
nrow(ha_water[which(ha_water$cond == 0.005), ])

# Most of the bad conductivity data comes from HA02 between 2019-07-20 and 2016-08-25
# From the sensor log, it appears that it was serviced on 07-20, then on 08-25 it was serviced again and noted that the conductivity probe was out of the water apparently hung up. 
nrow(ha_water[which(ha_water$ct < 0 & ha_water$site == "HA02"), ])
nrow(ha_water[which(ha_water$cond == 0.005 & ha_water$site == "HA02"), ])

# Water level cannot be negative
nrow(ha_water[which(ha_water$level_m < 0), ])

# Break up the data into DO, conductivity, and water level dataframes
# Remove the do saturation, I will calculate this later based on the cleaned data and using the function Bob developed.
ha_do    <- ha_water[, c('site', 'sensor_number', 'datetime', 'do_conc', 'do_temp')]
ha_cond  <- ha_water[, c('site', 'sensor_number', 'datetime', 'cond', 'cond_temp', 'ct')]
ha_level <- ha_water[, c('site', 'sensor_number', 'datetime', 'level_m', 'level_temp')]

# Most of the bad conductivity data comes from HA02 between 2016-07-20 and 2016-08-25
# From the sensor log, it appears that it was serviced on 07-20, then on 08-25 it was serviced again and noted that the conductivity probe was out of the water apparently hung up.
# Remove the conductivity from this site for this period of time.
ha_cond <- ha_cond[-which(ha_cond$ct < 0 & ha_cond$site == "HA02" &
                          ha_cond$datetime >= as.POSIXct('2016-07-20 00:00') &
                          ha_cond$datetime <= as.POSIXct('2016-08-25 23:00')), ]

ha_cond[which(ha_cond$ct < 0),]

# At this point, I need to start the process of breaking up the data, interpolating between missing data sources, and running the anomalize package to ID outliers, then add the data back with the potential outliers removed.
# Should start with the DO temperature data 

# When the RDO sensor is throwing NaN values, the temperature sensor seems to "freeze" and report the same number over and over or report a zero value. Based on the sensor service log, it appears that most of these prolonged instances occured because there was a kink in the RDO cord that had to be replaced before the sensor would begin report reasonable values again. For this reason, I will remove all data where the DO concentration is NaN.
ha_do <- ha_do[which(is.finite(ha_do$do_conc) == TRUE),]


# Compile a list where each list item is the data from a unique sensor number
ha_do_list <- split(ha_do, f = as.factor(ha_do$sensor_number))

# Because anomalous zero values are especially problematic for the gap-filling process, I am going to remove them from some of the sensors. I couldn't figure out a process to systematically remove these from sensors where these are anomalous versus part of the typical variation for the sensor.

# Sensors with anomalous zero values (as evaluated based on time series figures from each sensor): 1, 3, 4, 5, 7, 8
ha_do_list <- lapply(ha_do_list, function(x){
  if(x$sensor_number[1] == '01' | x$sensor_number[1] == '04' | 
     x$sensor_number[1] == '05'){
    x <- x[-which(x$do_temp <= 0), ]
  } else {x}
  return(x)
})


# Glitches in sensor DO data are really difficult to distinguish from flood data. I am removing these instances from sensor 9 because they are difficult for this reason. I determined that they are glitches in the RDO sensor based on visually inspecting the temperature data and comparing this to the other sensor data (water level, cond, other temp data) to evaluate for peak matching.
# Is the temp anomaly data from sensor 9 in 2017 really an outlier or a flood? Answer: Probably bad data, no evidence of flood from this period.

# Glitch 1
glitch1_srt <- as.POSIXct('2017-10-01 13:00')
glitch1_end <- as.POSIXct('2017-10-03 14:00')

# xlims <- as.POSIXct(c('2017-10-01 13:00', '2017-10-03 14:00'))
# xlims <- as.POSIXct(c('2017-09-30 00:00', '2017-10-05 23:00'))
# par(mfrow = c(2,1))
# plot(ha_do$datetime[ha_do$sensor_number=='09'],
#      ha_do$do_temp[ha_do$sensor_number=='09'],
#      xlim = xlims, , ylim =  c(7, 14))
# 
# plot(ha_level$datetime[ha_level$sensor_number=='09'],
#      ha_level$level_m[ha_level$sensor_number=='09'],
#      xlim = xlims, , ylim = c(1.75,1.95))

# Glitch 2
glitch2_srt <- as.POSIXct('2017-05-07 05:00')
glitch2_end <- as.POSIXct('2017-05-07 11:00')

# xlims <- as.POSIXct(c('2017-05-07 05:00', '2017-05-07 11:00'))
# xlims <- as.POSIXct(c('2017-05-07 00:00', '2017-05-07 23:00'))
# par(mfrow = c(2,1))
# plot(ha_do$datetime[ha_do$sensor_number=='09'],
#      ha_do$do_temp[ha_do$sensor_number=='09'],
#      xlim = xlims, , ylim =  c(3, 8))
# 
# plot(ha_level$datetime[ha_level$sensor_number=='09'],
#      ha_level$level_m[ha_level$sensor_number=='09'],
#      xlim = xlims, ylim = c(3.4, 3.6))

# Remove the glitches
ha_do_list[[9]] <- ha_do_list[[9]][-which(ha_do_list[[9]]$datetime >= glitch1_srt &
                                          ha_do_list[[9]]$datetime <= glitch1_end),]

ha_do_list[[9]] <- ha_do_list[[9]][-which(ha_do_list[[9]]$datetime >= glitch2_srt &
                                          ha_do_list[[9]]$datetime <= glitch2_end),]

# Gap filling
# Create a new list that has a row for every datetime between the first observation and last observation for each site. 
gap_fill <- lapply(ha_do_list, function(x){
  time_vec  <- seq(from = min(na.omit(x$datetime)), to = max(na.omit(x$datetime)), by = "1 hour")
  all_times <- data.frame(datetime = time_vec)
  return(all_times)
})

# Merge the do_temp data into the above list
gap_fill <- lapply(seq(gap_fill), function(i){
  merge(gap_fill[[i]], ha_do_list[[i]], by = "datetime", all.x = TRUE)
})

# Interpolate the missing temperature data, add a column to indicate whether the data are interpolated (helpful for plotting purposes)
gap_fill <- lapply(gap_fill, function(x){
  # Interpolate data
  x$do_temp_interp <- na_interpolation(x$do_temp, option = "linear")
  
  # Column to identify interpolated values
  x$interp <- 'no'
  x$interp[is.na(x$do_temp)] <- 'yes'
  
  # Return the whole list
  return(x)
})

# Convert the df to a tibble time object (required for the anomalize package).
gap_fill <- lapply(gap_fill, function(x){
  as_tbl_time(x, datetime)
})


# After a lot of trying to tune the alpha parameter, I think the best approach is to use a different alpha level for each well. This is because each well has a unique variability and some wells are more sensitive to influence from flood than others.
sensor <- c(paste('0',c(1:9),sep = ''), '10')
alpha  <- c(0.0005, #1
            0.010,  #2
            0.001,  #3
            0.002,  #4
            0.003,  #5
            0.015,  #6
            0.0025, #7
            0.010,  #8
            0.0015, #9
            0.015) #10

# Anomaly identification
# Run the anomaly detection process
anom_do_temp <- lapply(seq_along(gap_fill), function(i){
  gap_fill[[i]] %>%  
    time_decompose(do_temp_interp, 
                   method = "stl",
                   frequency = "24 hours",
                   trend = "2 weeks") %>%
    anomalize(remainder, 
              method = "iqr", 
              alpha = alpha[i]) %>%
    time_recompose()
})

# Merge the anomaly data into the gap_fill df
do_temp_anom <- lapply(seq_along(gap_fill), function(i){
  gap_fill[[i]]$anomaly <- anom_do_temp[[i]]$anomaly
  return(gap_fill[[i]])
}) 

# Visualize the results

# Notes about the interative process of finding something that works

# Based on a first pass of the analysis, it appears that the outlier zeros in the data at some (mostly groundwater) sensors are particularly problematic because the often occur following or proceeding a gap in observations. As a result, the gap filling process is making these outlier appear consisten with the larger trend. I will revise the analysis above to remove these zero temp outliers.

# Redo visualization with anomalous zeros removed from sensors 1, 3, 4, 5, 7, 8
# This appears to have improved the anomaly detection (fewer of the anomalous zero data points remaining unflagged due to gap filling normalizing these data points)

# Redo visualization after removing the instances where a faulty RDO cable was causing the sensor to record near constant temperature and "NaN" values for DO concentration in sensor 5 and 9.
# This appears to look good at this macro scale. I need to "zoom in" on each site to check that the anomaly detection process is catching actual outliers rather than typical variation in the data (due to floods, etc.).

# After "zooming in" I've found that this process flags temperature changes that happen durring floods, which can cause a sudden change in the well water temperature for up to a week. Because the trend information is calculated for a two-week period, it glosses over the flood data. I will try setting the trend component to a shorter period of time.

# Set the IQR value to 0.005 and trend to 2 days, but a lot of flood data is still being flagged as an outlier. I am now going to adjust the maximum anomalies value to try and reduce the number of observations flagged.

# Changing the max anoms to 0.0005 flagged mostly appropriate outliers for the wells, but it forced the process to ID outliers for the surface water sensors, often where there are none (such as for CASC, 06). Maybe take a tiered approach, where the surface water sensors have whatever IQR and alpha setting work for those sites and the well water uses the max_anoms functionality.

# After making a separate outlier detection process for the surface water and wells, it seems that the glitches in sensor 09 are tough to distinguish from flood data. I have removed these glitches to help with the tuning of the alpha parameter.

# Moving in the right direction! Some of the flood data from sensor 1 and 9 is still being flagged. Continue adjusting the alpha values.

# I think I really just have to have individual alpha values for each station. It's too difficult finding one value that fits each of the wells because they all have different variability. Some are heavily influenced by floods and other are not.

# Okay, alter tuning the alphas for each sensor, I believe we have a cleaned dataset! Now I need to recompose a dataframe with a sensible missing number value (-999?) for when the sensors glitched or were on the fritz. Then impliment a flag for data that is tagged as anomalous through this process.

y_label   <- "Temp (C)"

png("./Figures/DO_temp_anomaly_well_diff_alphas.png", units = "in", 
    width = 12, height = 12, res = 300, bg = "transparent")

par(mfrow = c(5,2), oma = c(2,2,1,1), mar = c(3,5,2,0))

lapply(do_temp_anom, function(x){
  # Plot raw data, without the interpolated data
  plot(x$datetime[x$interp == 'no'], x$do_temp[x$interp == 'no'], 
       xlab = " ", ylab = y_label, cex.lab = 2, cex.axis = 2, pch = 20, cex.pch = 2)

  # Highlight detected anomalies in raw data (not including interpolated data)
  points(x$datetime[x$anomaly == 'Yes' & x$interp == 'no'],
         x$do_temp[x$anomaly == 'Yes' & x$interp == 'no'],
         pch = 1, cex.pch = 2, col = 'red')
    
  title(paste(x[1,2], x[1,3], sep = '_'), 
        line = 0.5, cex.main = 2)
  box(lwd = 2)

  }
)

dev.off()

# Now look at individual sites to see how the anomaly detection is working. Are anomalies flagged? Are variable data due to flooding or other phase shifts falsely IDed as anomalies?

# Is the early data from sensor 01 OK? Yes, looks like floods
xlims <- as.POSIXct(c('2011-12-10 00:00', '2012-04-30 23:00'))
ylims <- c(3, 7)

anom_do_temp[[1]] %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = ylims) + 
  scale_x_datetime(limits = xlims)

par(mfrow = c(2,1))
plot(ha_do_list[[1]]$datetime, ha_do_list[[1]]$do_temp, ylim = ylims, xlim = xlims)
abline(v = as.POSIXct('2012-04-12 00:00'))
abline(v = as.POSIXct('2012-04-26 00:00'))

plot(ha_water$datetime[ha_water$sensor_number == '01'], 
     ha_water$level_m[ha_water$sensor_number == '01'], 
     xlim = xlims, ylim = c(2.4,3.1))
abline(v = as.POSIXct('2012-04-12 00:00'))
abline(v = as.POSIXct('2012-04-26 00:00'))

# Seems that most of this early variation is due to floods that caused the temp to drop in the well. I'm inclined to believe that these data are fine (with the exception of the two very high observations).

# What about later in the year?
xlims <- as.POSIXct(c('2014-06-20 00:00', '2014-07-02 23:00'))
ylims <- c(4.5, 20)
anom_do_temp[[1]] %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = ylims) + 
  scale_x_datetime(limits = xlims)

# Looks like there is more of that data where temp is constant and DO is probably NaN
# Also potentially more floods
xlims <- as.POSIXct(c('2013-03-01 00:00', '2013-04-30 23:00'))
ylims <- c(4.5, 5.5)
anom_do_temp[[1]] %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = ylims) + 
  scale_x_datetime(limits = xlims)

par(mfrow = c(2,1))
plot(ha_do_list[[1]]$datetime, 
     ha_do_list[[1]]$do_temp, ylim = ylims, xlim = xlims)

plot(ha_water$datetime[ha_water$sensor_number == '01'], 
     ha_water$level_temp[ha_water$sensor_number == '01'], 
     xlim = xlims, , ylim = c(4.5,5.5))

xlims <- as.POSIXct(c('2013-04-08 00:00', '2013-04-25 23:00'))
ylims <- c(4.4, 5.2)
anom_do_temp[[1]] %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = ylims) + 
  scale_x_datetime(limits = xlims)

# More issues associated with floods but are actually totally fine data (I think)
# Grr, really wish I could figure out a correlated variable outlier analysis. Using the water level data from the same sight would probably address this issue.

test <- lm(do_temp ~ level_m, data = ha_water[ha_water$sensor_number == '01',]); summary(test)
test <- lm(do_temp ~ level_temp, data = ha_water[ha_water$sensor_number == '01',]); summary(test)

# Actually, it appears that the relationship between water level and do temperature is not all that great

# Are there similar flood issues at other sites?

# Sensor 3
xlims <- as.POSIXct(c('2018-07-13 00:00', '2018-07-20 23:00'))
ylims <- c(5.6, 5.9)

anom_do_temp[[3]] %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = ylims) + 
  scale_x_datetime(limits = xlims)

plot(ha_level$datetime[ha_level$sensor_number=='03'], ha_level$level_m[ha_level$sensor_number=='03'], xlim = xlims, ylim = c(3.2,3.4))

plot(ha_level$datetime[ha_level$sensor_number=='03'], ha_level$level_temp[ha_level$sensor_number=='03'], xlim = xlims, ylim = c(5.6,5.9))
# This data blip is likely due to a sensor glitch during a change in the program on 2018-07-16

xlims <- as.POSIXct(c('2018-05-01 00:00', '2018-07-05 23:00'))
ylims <- c(4, 10)

anom_do_temp[[3]] %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = ylims) + 
  scale_x_datetime(limits = xlims)

# Sensor 4
xlims <- as.POSIXct(c('2012-03-20 00:00', '2012-07-01 23:00'))
ylims <- c(3, 6.5)

anom_do_temp[[4]] %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = ylims) + 
  scale_x_datetime(limits = xlims)

xlims <- as.POSIXct(c('2012-03-25 00:00', '2012-04-15 23:00'))
ylims <- c(3, 4.1)

anom_do_temp[[4]] %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = ylims) + 
  scale_x_datetime(limits = xlims)

plot(ha_level$datetime[ha_level$sensor_number=='04'], ha_level$level_m[ha_level$sensor_number=='04'], xlim = xlims, ylim = c(2.1,2.35))

length(do_temp_anom[[4]]$anomaly[do_temp_anom[[4]]$anomaly == "Yes"])

# This anomaly detection process performs really poorly with the floods affecting the data. This is probably because the trend is set to two weeks, which is long enough to flag a 4-7 day flood.

# Is the really low temperature data from sensor 8 in 2015 really an outlier or a flood? Answer: flood, fo sho.
xlims <- as.POSIXct(c('2015-03-15 00:00', '2015-03-18 23:00'))
ylims <- c(0, 6.5)

anom_do_temp[[8]] %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = ylims) + 
  scale_x_datetime(limits = xlims)

plot(ha_level$datetime[ha_level$sensor_number=='08'], ha_level$level_m[ha_level$sensor_number=='08'], xlim = xlims, ylim = c(.5,1.15))
abline(v = as.POSIXct('2015-03-16 00:00'))
abline(v = as.POSIXct('2015-03-18 00:00'))

# Is the temp anomaly data from sensor 9 in 2017 really an outlier or a flood? Answer: Probably bad data, no evidence of flood from this period.
xlims <- as.POSIXct(c('2017-09-30 00:00', '2017-10-05 23:00'))
ylims <- c(7, 14)

anom_do_temp[[9]] %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = ylims) + 
  scale_x_datetime(limits = xlims)

plot(ha_level$datetime[ha_level$sensor_number=='09'], ha_level$level_m[ha_level$sensor_number=='09'], xlim = xlims, , ylim = c(1.75,1.95))
abline(v = as.POSIXct('2017-10-01 00:00'))
abline(v = as.POSIXct('2017-10-04 00:00'))

plot(ha_level$datetime[ha_level$sensor_number=='09'], ha_level$level_temp[ha_level$sensor_number=='09'], xlim = xlims, ylim = c(13.45,13.85))

plot(ha_cond$datetime[ha_level$sensor_number=='09'], ha_cond$cond[ha_level$sensor_number=='09'], xlim = xlims, ylim = c(.2,.25))


# Another glitch in sensor 09
xlims <- as.POSIXct(c('2017-05-06 00:00', '2017-05-8 23:00'))
ylims <- c(3, 8)

anom_do_temp[[9]] %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = ylims) + 
  scale_x_datetime(limits = xlims)

plot(ha_level$datetime[ha_level$sensor_number=='09'], ha_level$level_temp[ha_level$sensor_number=='09'], xlim = xlims)

plot(ha_level$datetime[ha_level$sensor_number=='09'], ha_level$level_m[ha_level$sensor_number=='09'], xlim = xlims, ylim = c(3,3.5))

plot(ha_cond$datetime[ha_level$sensor_number=='09'], ha_cond$cond[ha_level$sensor_number=='09'], xlim = xlims)

# Flood
xlims <- as.POSIXct(c('2013-06-05 00:00', '2013-06-10 23:00'))
ylims <- c(5, 10)

anom_do_temp[[9]] %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = ylims) + 
  scale_x_datetime(limits = xlims)

plot(ha_level$datetime[ha_level$sensor_number=='09'], ha_level$level_m[ha_level$sensor_number=='09'], xlim = xlims, ylim = c(3.2,3.6))

# Is the temp anomaly data from sensor 1 in 2014 really an outlier or a flood? Answer: Not a flood, looks like some kind of glitch across sensors. Probably bad data.
xlims <- as.POSIXct(c('2014-06-26 00:00', '2014-06-30 23:00'))
ylims <- c(4, 20)

anom_do_temp[[1]] %>%
  plot_anomalies(time_recomposed = TRUE) +
  scale_y_continuous(limits = ylims) + 
  scale_x_datetime(limits = xlims)

plot(ha_level$datetime[ha_level$sensor_number=='01'], ha_level$level_m[ha_level$sensor_number=='01'], xlim = xlims)

plot(ha_level$datetime[ha_level$sensor_number=='01'], ha_level$level_temp[ha_level$sensor_number=='01'], xlim = xlims)

plot(ha_cond$datetime[ha_level$sensor_number=='01'], ha_cond$cond[ha_level$sensor_number=='01'], xlim = xlims)

plot(ha_cond$datetime[ha_level$sensor_number=='01'], ha_cond$cond_temp[ha_level$sensor_number=='01'], xlim = xlims)

```

Filed under: Things I tried and didn't work out.

```{r}

# Based on a first pass of the analysis, it appears that the outlier zeros in the data at some (mostly groundwater) sensors are particularly problematic because the often occur following or proceeding a gap in observations. As a result, the gap filling process is making these outlier appear consisten with the larger trend. I will revise the analysis above to remove these zero temp outliers.

# The process I tried below failed to flag these zero observations and also flagged some observations that are likley consistent with the data (especially high temperature data).

# Experiment with removing outlier zeros from the dataframes
# Try removing data >3 SD from the mean
mean_sd <- lapply(ha_do_list, function(x){
  mean_temp <- mean(na.omit(x$do_temp))
  sd_temp   <- sd(na.omit(x$do_temp))
  max_cut   <- mean_temp + 4 * sd_temp
  min_cut   <- mean_temp - 4 * sd_temp
  return(c(mean_temp, sd_temp, max_cut, min_cut))
})

# Flag values that are above/below 4 SD from the mean
cutoff <- lapply(seq_along(ha_do_list), function(i){
  ha_do_list[[i]]$outlier <- "no"
  ha_do_list[[i]]$outlier[ha_do_list[[i]]$do_temp > mean_sd[[i]][3]] <- "yes"
  ha_do_list[[i]]$outlier[ha_do_list[[i]]$do_temp < mean_sd[[i]][4]] <- "yes"
  return(ha_do_list[[i]])
})

# Plot it out
y_label   <- "Temp (C)"

png("./Figures/DO_temp_rm4sd.png", units = "in", 
    width = 12, height = 12, res = 300, bg = "transparent")

par(mfrow = c(5,2), oma = c(2,2,1,1), mar = c(3,5,2,0))

lapply(seq_along(cutoff), function(i){
  # Plot raw data
  plot(cutoff[[i]]$datetime, cutoff[[i]]$do_temp, xlab = " ", ylab = y_label, 
       cex.lab = 2, cex.axis = 2, pch = 20, cex.pch = 2)

  # Highlight points that are beyond 4 sds from the mean
  points(cutoff[[i]]$datetime[cutoff[[i]]$outlier == 'yes'],
         cutoff[[i]]$do_temp[cutoff[[i]]$outlier == 'yes'],
         pch = 1, cex.pch = 2, col = 'red')
    
  title(paste(cutoff[[i]][1,2], cutoff[[i]][1,3], sep = '_'), 
        line = 0.5, cex.main = 2)
  box(lwd = 2)

  }
)

dev.off()

```


Outlier identification with methods that account for missing data
sltplus package

```{r}
# sltplus package uses interpolation for missing data (maybe has to be NA values?)
# devtools::install_github("hafen/stlplus")

library(stlplus)

# Following this tutorial
# http://www.gardner.fyi/blog/STL-Part-I/

# Try implementing "seasonal" decomposition for daily data
do_temp_2017 <- do_temp[do_temp$datetime >= as.POSIXct('2017-01-01 00:00') &  do_temp$datetime <= as.POSIXct('2017-12-29 23:00'),]

do_temp_2017 <- do_temp_2017[order(do_temp_2017$datetime),]

# This DF does not have the missing datetime info, create a new df with all datetime information and merge the DO data into this new df
temp <- seq(from = as.POSIXct('2017-01-01 00:00'), to = as.POSIXct('2017-12-31 23:00'), by = '1 hour')
temp <- data.frame(datetime = temp)

temp <- merge(temp, do_temp_2017, by = "datetime", all.x = TRUE)

do_temp_2017 <- temp

stlDaily <- stlplus(do_temp_2017$do_temp, t = do_temp_2017$datetime,
                    n.p = 24, s.window = "period",
                    t.window = 720)

plot(stlDaily, xlab="Date", ylab="Temp (C)")

# find the inner quartile range of the remainder
quants <- unname(quantile(stlDaily$data$remainder, na.rm = TRUE))

iqr <- quants[4] - quants[2]

iqr_3x <- iqr * 3

plot(stlDaily$time, stlDaily$data$remainder, ylim = c(-2*iqr_3x,2*iqr_3x), pch = ".")
abline(h = iqr_3x)
abline(h = -iqr_3x)

# Flag observations beyond 3x the iqr
stlDaily$data$outlier <- "no"
stlDaily$data$outlier[stlDaily$data$remainder > iqr_3x] <- "yes"
stlDaily$data$outlier[stlDaily$data$remainder < -iqr_3x] <- "yes"

# with(stlDaily$data[stlDaily$data$outlier == "no",], plot(stlDaily$time, stlDaily$data$remainder, ylim = c(-2*iqr_3x,2*iqr_3x), pch = ".")

out <- as.data.frame(stlDaily$data)
out$time <- stlDaily$time

# Plot remainder
plot(out$time, out$remainder, ylim = c(-2*iqr_3x,2*iqr_3x), pch = ".")
abline(h = iqr_3x)
abline(h = -iqr_3x)
points(out$time[out$outlier == "yes"], out$remainder[out$outlier == "yes"], pch = 19, col = "red")

# Plot raw values
xlims <- as.POSIXct(c('2017-04-01 00:00', '2017-04-05 00:00'))

plot(out$time, out$raw, pch = ".", ylim = c(5.2,5.4), xlim = xlims)
points(out$time[out$outlier == "yes"], out$raw[out$outlier == "yes"], pch = 19, col = "red", cex = 0.5)





par(mar = c(3,3,1,1))

plot(stlDaily$data$seasonal[7000:7922], pch = 20)
plot(stlDaily$data$trend[7000:7922], pch = 20)
plot(stlDaily$data$remainder[7000:7922], pch = 20)

plot(stlDaily$data$seasonal[7000:7922], pch = 20)
plot(stlDaily$data$trend[7000:7922], pch = 20)
plot(stlDaily$data$remainder[7000:7922], pch = 20)

plot(do_temp_2017$datetime[7000:7922], do_temp_2017$do_temp[7000:7922], pch = 20)

par(new=T)
plot(do_temp_2017$do_temp[7000:7922], pch = 20)


plot(stlDaily$data$raw[7000:7922], pch = 20)

temp <- do_temp_2017[do_temp_2017$datetime >= as.POSIXct('2017-10-01 00:00'),]

stlDaily <- stlplus(temp$do_temp, t = temp$datetime,
                    n.p = 24, s.window = 720)

plot(stlDaily, xlab="Date", ylab="Temp (C)")

plot(temp$datetime, temp$do_temp)

```


Visualize the 2013-present data

Explore the continuity in the data by plotting the whole record

Dissolved oxygen data

```{r}

# If I want to use the markdown capabilities
# echo = FALSE, fig.width = 4, fig.height = 3, res = 300, fig.cap = "DO concentration by well."

# To compile the water data from each site, exclude the meterological data
x <- ha_files[2:8]

png("./Figures/HA_DO_Conc.png", units = "in", 
    width = 12, height = 12, res = 300, bg = "transparent")

par(mfrow = c(5,2), oma = c(2,2,1,1), mar = c(3,5,2,0))

lapply(seq_along(x), function(i){
  if(ncol(x[[i]]) == 10) {
  plot(x[[i]][,1], x[[i]][,3], xlab = " ", ylab = "DO (mg/L)", 
       cex.lab = 2, cex.axis = 2)
  title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[3], sep = ' '), 
        line = 0.5, cex.main = 2)
  box(lwd = 2)
  } else {
       plot(x[[i]][,1], x[[i]][,3], xlab = " ", ylab = "DO (mg/L)", 
       cex.lab = 2, cex.axis = 2)
       title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[3], sep = ' '), 
             line = 0.5, cex.main = 2)
       box(lwd = 2)
      
       plot(x[[i]][,1], x[[i]][,11], xlab = " ", ylab = "DO (mg/L)", 
       cex.lab = 2, cex.axis = 2)
       title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[11], sep = ' '), 
       line = 0.5, cex.main = 2)
       box(lwd = 2)
  }
})

dev.off()
```


Conductivity Data

```{r}
# x <- ha_files[2:8]

y_label   <- "Cond (mS/cm)"
first_col <- 6
sec_col   <- 14

png("./Figures/HA_Cond.png", units = "in", 
    width = 12, height = 12, res = 300, bg = "transparent")

par(mfrow = c(5,2), oma = c(2,2,1,1), mar = c(3,5,2,0))

lapply(seq_along(x), function(i){
  if(ncol(x[[i]]) == 10) {
  plot(x[[i]][,1], x[[i]][,first_col], xlab = " ", ylab = y_label, 
       cex.lab = 2, cex.axis = 2)
  title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[first_col], sep = ' '), 
        line = 0.5, cex.main = 2)
  box(lwd = 2)
  } else {
       plot(x[[i]][,1], x[[i]][,first_col], xlab = " ", ylab = y_label, 
       cex.lab = 2, cex.axis = 2)
       title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[first_col], sep = ' '),
             line = 0.5, cex.main = 2)
       box(lwd = 2)
      
       plot(x[[i]][,1], x[[i]][,sec_col], xlab = " ", ylab = y_label, 
       cex.lab = 2, cex.axis = 2)
       title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[sec_col], sep = ' '), 
       line = 0.5, cex.main = 2)
       box(lwd = 2)
  }
})

dev.off()

```


Water level

```{r}
# x <- ha_files[2:8]

y_label   <- "Water Level (m)"
first_col <- 9
sec_col   <- 17

png("./Figures/HA_WaterLevel.png", units = "in", 
    width = 12, height = 12, res = 300, bg = "transparent")

par(mfrow = c(5,2), oma = c(2,2,1,1), mar = c(3,5,2,0))

lapply(seq_along(x), function(i){
  if(ncol(x[[i]]) == 10) {
  plot(x[[i]][,1], x[[i]][,first_col], xlab = " ", ylab = y_label, 
       cex.lab = 2, cex.axis = 2)
  title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[first_col], sep = ' '), 
        line = 0.5, cex.main = 2)
  box(lwd = 2)
  } else {
       plot(x[[i]][,1], x[[i]][,first_col], xlab = " ", ylab = y_label, 
       cex.lab = 2, cex.axis = 2)
       title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[first_col], sep = ' '),
             line = 0.5, cex.main = 2)
       box(lwd = 2)
      
       plot(x[[i]][,1], x[[i]][,sec_col], xlab = " ", ylab = y_label, 
       cex.lab = 2, cex.axis = 2)
       title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[sec_col], sep = ' '), 
       line = 0.5, cex.main = 2)
       box(lwd = 2)
  }
})

dev.off()

```


Temperature associated with the DO sensor

```{r}
# x <- ha_files[2:8]

y_label   <- "Temp (C)"
first_col <- 5
sec_col   <- 13

png("./Figures/HA_DO_Temp_ylim20.png", units = "in", 
    width = 12, height = 12, res = 300, bg = "transparent")

par(mfrow = c(5,2), oma = c(2,2,1,1), mar = c(3,5,2,0))

lapply(seq_along(x), function(i){
  if(ncol(x[[i]]) == 10) {
  plot(x[[i]][,1], x[[i]][,first_col], xlab = " ", ylab = y_label, 
       cex.lab = 2, cex.axis = 2, ylim = c(0,20))
  title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[first_col], sep = ' '), 
        line = 0.5, cex.main = 2)
  box(lwd = 2)
  } else {
       plot(x[[i]][,1], x[[i]][,first_col], xlab = " ", ylab = y_label, 
       cex.lab = 2, cex.axis = 2, ylim = c(0,20))
       title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[first_col], sep = ' '),
             line = 0.5, cex.main = 2)
       box(lwd = 2)
      
       plot(x[[i]][,1], x[[i]][,sec_col], xlab = " ", ylab = y_label, 
       cex.lab = 2, cex.axis = 2, ylim = c(0,20))
       title(paste(substr(names(x)[[i]], 1, 4), colnames(x[[i]])[sec_col], sep = ' '), 
       line = 0.5, cex.main = 2)
       box(lwd = 2)
  }
})

dev.off()

```

EPA Job Talk Figure

```{r}

vis <- ha_files$HA07_water_2013_04_01_to_2019_06_25

png("./Figures/EPA Job Talk Fig.png", units = "in", 
    width = 5, height = 8, res = 300, bg = "transparent")

par(mfrow = c(4,1), oma = c(2,2,1,1), mar = c(3,5,2,0), c(3, 2, 0))

plot(vis$TIMESTAMP, vis$Lvl_m3, xlab = " ", ylab = "Water Level", 
       cex.lab = 2, cex.axis = 2, ylim = c(3,4.5), pch = 20, xaxt='n')
  title("Water Level (m)", line = 0.5, cex.main = 2)
  box(lwd = 2)
  
plot(vis$TIMESTAMP, vis$DO_W_T3, xlab = " ", ylab = "Temp", 
       cex.lab = 2, cex.axis = 2, ylim = c(4,10), pch = 20, xaxt='n')
  title("Temperature (°C)", line = 0.5, cex.main = 2)
  box(lwd = 2)
  
plot(vis$TIMESTAMP, vis$Cond3, xlab = " ", ylab = "Cond", 
       cex.lab = 2, cex.axis = 2, ylim = c(.2,.3), pch = 20, xaxt='n')
  title("Conductivity (mS)", line = 0.5, cex.main = 2)
  box(lwd = 2)
  
plot(vis$TIMESTAMP, vis$DoConc3, xlab = " ", ylab = "DO", 
       cex.lab = 2, cex.axis = 2, ylim = c(4,8), pch = 20)
  title("Dissolved Oxygen (mg/L)", line = 0.5, cex.main = 2)
  box(lwd = 2)  

dev.off()


```

