---
title: "Nyack Floodplain Data Archiving"
author: "Amalia Handler"
date: "6/24/2019"
output: 
  html_document: 
    keep_md: yes
editor_options: 
  chunk_output_type: console
---

Code to compile all River Net Nyack Floodplain meterological and hydrologic data. Code was written by Amalia Handler, Environmental Data Initiative Summer Data Science Fellow, during summer 2019. Please contact her with any questions: amalia.handler@gmail.com

Load data from FLBS internal database. It extends from 2013-04-01 to 2019-06-25. 

```{r include = FALSE}
# List the files in the directory from Phil
ha_files_toread <- list.files(path = "./Data", pattern = ".dat")

# Read in the files
toread <- paste("./Data/", ha_files_toread, sep = '')

ha_files <- list()
ha_files <- lapply(toread, read.delim, header = FALSE, sep = ',', skip = 4) 

# Get the column names
var_names <- list()
for(i in 1:length(ha_files)){
  var_names[[i]] <- colnames(read.delim(paste("./Data/", ha_files_toread[i], sep = ''), sep = ",", skip = 1, header = TRUE))
}

# Apply the column names to the list of dfs
for(i in 1:length(var_names)){
  colnames(ha_files[[i]]) <- var_names[[i]]
}

# Convert the timestamp to a datetime variable
ha_files <- lapply(ha_files, function(x){
 x$TIMESTAMP <- as.POSIXct(x$TIMESTAMP, format = "%Y-%m-%d %H:%M:%S")
 return(x)
})

# Determine if each file is a water or meteorlogical file
file_info <- sapply(ha_files, function(x){
  if(colnames(x)[3] == "AirTC") {
    data_type <- "met"
  } else { data_type <- "water"}
  return(data_type)
})

# Find the ha number for each file
# If the file contains "CR1000", then I can extract
# If the file name contains "Nyack" then need another method
well_info <- NULL
for(i in 1:length(ha_files_toread)){
  if(grepl("CR1000", ha_files_toread[i])){
  well_info[i] <- substr(ha_files_toread[i], 8, 11)
  } else {well_info[i] <- substr(ha_files_toread[i], 7, 10)}
}

# Name the dataframes in the list
ha_df_names <- paste(well_info, file_info, sep = '_')

names(ha_files)  <- ha_df_names
names(var_names) <- ha_df_names

# Need to coerce the mutiple columns in the HA02 df from character to number. Seems that R does not see "NAN" is equivelent to "NaN".
column_numbers <- c(11, 14:17)
for(col in column_numbers){
  ha_files$HA02_water[,col] <- 
    as.numeric(ha_files$HA02_water[,col])
}

```


Compile data collected prior to 2013-04-01

Often, these data we manually retrieved from each sensor prior to when these sensors were connected to the remote data download system through the cabin at HA08.

By going through the sensor service log for each station, I found the date that each station was confirmed as collecting reliable data (excludes test data).

Station, start date
CASC, 2012-09-06
HA02, 2012-06-08
HA07, 2012-05-15 (both met station and well)
HA08, 2011-12-14 (well)
HA08, 2012-05-02 (Beaver Creek)
HA10, 2011-12-16
HA12, 2012-04-26
HA15, 2012-04-26

# Append this older data to the longer data compiled in the code chunk above.

```{r}

# Find all the initial data files with data between the start of monitoring and the start of the data from the FLBS database (2013-04-01).
ini_files <- c(
  './Data/Nyack_RiverNET_HA02 Movie Rd/Nyack HA02 MovRd_Table1 5_8_12 thru 3_31_13.dat',
  './Data/Nyack_RiverNET_HA07 MET/CR1000_HA07_Met_5_15_12 thru 7_6_12.dat',
  './Data/Nyack_RiverNET_HA07 MET/CR1000_HA07_Met 7_6_12 thru 3_31_13.dat',
  './Data/Nyack_RiverNET_HA07 MET/CR1000_HA07_Water_5_15_12 thru 7_6_12.dat',
  './Data/Nyack_RiverNET_HA07 MET/CR1000_HA07_Water 7_6_12 thru 3_31_13.dat',
  './Data/Nyack_RiverNET_HA08_Cabin/Manual download/Nyack HA08 Cabin_Table1 5_2_12 well only.dat',
  './Data/Nyack_RiverNET_HA08_Cabin/Manual download/Nyack HA08 Cabin_Table1 5_2_12 thru 11_28_12.dat',
  './Data/Nyack_RiverNET_HA08_Cabin/CR1000_HA08 Cabin_Table1 8_23_12 thru 3_31_13.dat',
  './Data/Nyack_RiverNET_HA10_Sarg/CR1000_HA10_Table1 12_16_11 thru 3_31_13.dat',
  './Data/Nyack_RiverNET_HA12_Methane/CR1000_HA12_Table1 4_26_12 thru 3_31_13.dat',
  './Data/Nyack_RiverNET_HA15_Springhead/CR1000_HA15_Table1 4_26_12 thru 3_31_13.dat',
  './Data/Nyack_RiverNET_Cascadilla/Nyack CASC_Table1 9_6_12 thru 3_31_13.dat')

ini_ha_files <- lapply(ini_files, read.delim, header = FALSE, sep = ',', skip = 4) 

# Get the column names
var_names <- list()
for(i in 1:length(ini_ha_files)){
  var_names[[i]] <- colnames(read.delim(ini_files[i], sep = ",", skip = 1, header = TRUE))
}

# Apply the column names to the list of dfs
for(i in 1:length(var_names)){
  colnames(ini_ha_files[[i]]) <- var_names[[i]]
}

# Convert the timestamp to a datetime variable
ini_ha_files <- lapply(ini_ha_files, function(x){
 x$TIMESTAMP <- as.POSIXct(x$TIMESTAMP, format = "%Y-%m-%d %H:%M:%S")
 return(x)
})

# Apply names to the dataframes
names(ini_ha_files) <- c('ha02', 'ha07met', 'ha07met', 'ha07wtr', 'ha07wtr', 'ha08', 'ha08', 'ha08', 'ha10', 'ha12', 'ha15', 'casc')

# Coerce one of the column from one of the HA08 files from character to numeric
ini_ha_files[[6]]$DoConc2 <- as.numeric(ini_ha_files[[6]]$DoConc2)

# Bind together rows from the same station
ini_ha07met <- rbind(ini_ha_files[[2]], ini_ha_files[[3]])
ini_ha07wtr <- rbind(ini_ha_files[[4]], ini_ha_files[[5]])
ini_ha08    <- rbind(ini_ha_files[[6]], ini_ha_files[[7]], ini_ha_files[[8]])

# Remove duplicates from the above bound files
ini_ha07met <- ini_ha07met[!duplicated(ini_ha07met),]
ini_ha07wrt <- ini_ha07wtr[!duplicated(ini_ha07wtr),]
ini_ha08    <- ini_ha08[!duplicated(ini_ha08),]

# Make a new list, now with the bound, duplicates removed data
ini_ha_files <- list(ini_ha_files$casc, 
                     ini_ha_files$ha02, 
                     ini_ha07met, 
                     ini_ha07wtr, 
                     ini_ha08, 
                     ini_ha_files$ha10, 
                     ini_ha_files$ha12, 
                     ini_ha_files$ha15)

# Bind these earlir data files to the data compiled previously

# First order the data
ha_files <- ha_files[order(names(ha_files))]

# Now bind together
ha_comb <- list()
for(i in 1:length(ha_files)){
  ha_comb[[i]] <- rbind(ini_ha_files[[i]], ha_files[[i]])
}

# Name the dfs
names(ha_comb) <- c('casc', 'ha02', 'ha07met', 'ha07wtr', 'ha08', 'ha10', 'ha12', 'ha15')


# Rename "timestamp" to "datetime" column and remove the records column in the met data
# Add a site column
ha_comb$ha07met$site          <- "ha07" 
colnames(ha_comb$ha07met)[1]  <- "datetime"
ha_comb$ha07met               <- ha_comb$ha07met[, c(21,1,3:20)]

# Remove duplicated data
ha_comb$ha07met <- ha_comb$ha07met[!duplicated(ha_comb$ha07met),]

# Remove rows with no datetime information (artifact of changing over to daylight savings time)
ha_comb$ha07met <- ha_comb$ha07met[!is.na(ha_comb$ha07met),]

# Save the meterological data file
saveRDS(ha_comb$ha07met, './Data/Nyack_Met_Data.rds')
write.csv(ha_comb$ha07met, './Data/Nyack_Met_Data.csv', row.names = FALSE)

```


Compile all water data into one dataframe.


```{r}
# For HA02, 08, and 15, there are two sets of sensors. Need to separate these into different dataframes.
#   HA02 has sensors 9 and 10
#   HA08 has sensors 1 and 2
#   HA15 has sensors 7 and 8
ha02_09 <- ha_comb$ha02[,colnames(ha_comb$ha02)[1:10]]
ha02_10 <- ha_comb$ha02[,colnames(ha_comb$ha02)[c(1,2,11:18)]]
ha08_01 <- ha_comb$ha08[,colnames(ha_comb$ha08)[1:10]]
ha08_02 <- ha_comb$ha08[,colnames(ha_comb$ha08)[c(1,2,11:18)]]
ha15_07 <- ha_comb$ha15[,colnames(ha_comb$ha15)[1:10]]
ha15_08 <- ha_comb$ha15[,colnames(ha_comb$ha15)[c(1,2,11:18)]]

# HA08_02 didn't come online until 2012-05-02. Need to trim this df to only include dates greater than and including this date. From visually inspecting the dataframe, it looks like the first "real" numbers appear at 2012-05-02 11:00:00.

# TO avoid headaches with the rows that are missing datetime information, and because the df is organized in chronological order, I'll use a match function rather than a logical (>) to select the observations that come after 2012-05-02.
first_row <- match(as.POSIXct('2012-05-02 11:00:00'), ha08_02$TIMESTAMP)
ha08_02   <- ha08_02[first_row:nrow(ha08_02),] 

# HA02_10 was initially installed in a parafluvial zone, but quickly this zone dried out just a few months after installation and was removed. It was removed on 2012-09-21 around 15:00. Note that the sensor was dry for some time before it was removed (first noted as dry on 2012-07-12 from the sensor service log).
# Based on visual inspection of the data, the water level switches from positive to negative at 2012-07-11 11:00:00
# Remove data for this site after this point in time
last_row <- match(as.POSIXct('2012-07-11 11:00:00'), ha02_10$TIMESTAMP)
ha02_10  <- ha02_10[1:last_row,]

# Recombine into a list
ha_sep <- list(ha_comb$casc, ha02_09, ha02_10, ha_comb$ha07wtr,
               ha08_01, ha08_02, ha_comb$ha10, ha_comb$ha12,
               ha15_07, ha15_08)

# Set the site and sensor numbers
site <- c('CASC', 'HA02', 'HA02', 'HA07', 'HA08', 'HA08', 'HA10', 'HA12', 'HA15', 'HA15')

sensor <- sapply(ha_sep, function(x){
  col_name <- colnames(x)[10]
  if(nchar(col_name) == 8){
    num <- substr(col_name, 8, 8)
    sensor_number <- paste('0', num, sep = '')
  } else {
    sensor_number <- substr(col_name, 8, 9)
  }
  return(sensor_number)
})


# Add columns to each df for the site and the sensor number
for(i in 1:length(ha_sep)){
  ha_sep[[i]]$site <- site[i]
  ha_sep[[i]]$sensor_number <- sensor[i]
}

# Now bind together all the files to form one very long dataframe.
# Need to rename all the columns first for compatabiility
# New column names
col_names <- c('datetime', 'record', 'do_conc', 'do_sat', 'do_temp', 'cond', 'ct', 'cond_temp', 'level_m', 'level_temp', 'site', 'sensor_number')

# Rename the columns for the dfs in the list
ha_rename <- lapply(ha_sep, function(x){
  colnames(x) <- col_names
  return(x)
})

# Bind together all rows
ha_all <- do.call("rbind", ha_rename)

# Rearrange the columns
ha_water <- ha_all[, c(11,12,1,3:10)]

# Remove duplicated rows
ha_water <- ha_water[!duplicated(ha_water),]

# Save the new master file
saveRDS(ha_water, './Data/HA_Water_Data_Compiled.rds')
write.csv(ha_water, './Data/HA_Water_Data_Compiled.csv', row.names = FALSE, quote = FALSE)

```

Voila! All River Net data are compiled.
