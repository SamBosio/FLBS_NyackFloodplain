---
title: "Nyack_Floodplain_Flood_Finder"
author: "Amalia Handler"
date: "5/12/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

How to find a flood on a river

Potential things to try
- A threshold associated with the 1st derivative
- Use the anomalize package that was used to detect bad data

Questions/values to help get started
- What are typical 1st derivative values associated with a flood rising limb?
    - The geometric mean is 0.0092
    - The arithmetic mean is 0.0121
- What is the typical window of time used to consider a flood?
    - The rising limb varies between 24 and 60 hours
    - The windows considered so far are 200-400 hours or 8-16 days

```{r}
# The data
level <- read.csv('./Data/Nyack_level_cleaned.csv', header = TRUE, stringsAsFactors = FALSE)

# Remove questionable and bad data
level <- level[-which(level$level_m == -999 | level$level_m_flag == 1 | level$level_m_flag == 2),]

# Convert datetime to POSIX variable type
level$datetime <- as.POSIXct(level$datetime, tz = "MST")

# Rank groundwater wells by distance from the Middle Fork of the Flathead River
# The water level on the Middle Fork (CASC sensor) receives a rank of 1
sensor <- c(6,9,5,7,4,1,3)
rank   <- c(1:7)
rank   <- data.frame(sensor, rank)
level  <- merge(level, rank, by.x = "sensor_number", by.y = "sensor")

# Subset the water level data to only include the sensors for this analysis
level <- level[level$sensor_number %in% sensor,]

# Break the df into a list based on rank
level <- split(level, f = as.factor(level$rank))

# Make the names the sensor number
names(level) <- sensor

# Order each df by the datetime
level <- lapply(level, function(x){
  x[order(x$datetime),]
})

```

Start out by considering one year of data that has floods I've already identified (2014)

Calculate the 1st derivative for the whole time series for the main stem

Flag all instances where the 1st derivative exceeds the threshold (make it so the threshold is easily changable)

```{r}
library(signal)

# SUbset to 2014
lvl_14 <- level[[1]][level[[1]]$datetime >= as.POSIXct('2014-01-01') & level[[1]]$datetime <= as.POSIXct('2014-12-31'),]

# Savitsky-Golay filter settings
# p - order of polynomial line fit
# n - number of observations in the window (must be odd)
# m - derivative order to be returned of the fitted line
sg <- sgolay(p = 3, n = 17, m = 0)
sgd <- sgolay(p = 3, n = 17, m = 1)

# Calculate the 1st derivative of the main stem water level
m_derv <- filter(sgd, lvl_14$level_m)

# Set the threshold for flagging obsevations of the first derivative
TH <- 0.0092

# Flag observations over the threshold
lvl_14$flood_flag <- 0
lvl_14$flood_flag[m_derv >= TH] <- 1

# Plot it out
par(mar = c(4,4,1,1), mfrow = c(1,1))
plot(lvl_14$datetime, lvl_14$level_m, pch = 19, cex = 0.5)
points(lvl_14$datetime[m_derv >= TH], lvl_14$level_m[m_derv >= TH], pch = 19, cex = 0.5, col = 'blue')

# Plot a smaller time-chunk
lvl_14$derv <- m_derv
lvl_14$smooth <- filter(sg, lvl_14$level_m)
tmp <- lvl_14[lvl_14$datetime >= as.POSIXct('2014-02-25') & lvl_14$datetime <= as.POSIXct('2014-03-05'),]

par(mfrow = c(2,1), mar = c(2,2,1,1))
plot(tmp$datetime, tmp$level_m, pch = 19, cex = 0.5)
points(tmp$datetime[tmp$derv >= TH], tmp$level_m[tmp$derv >= TH], pch = 19, cex = 0.5, col = 'blue')
lines(tmp$datetime, tmp$smooth)

plot(tmp$datetime, tmp$derv, pch = 19, cex = 0.5)
points(tmp$datetime[tmp$derv >= TH], tmp$derv[tmp$derv >= TH], pch = 19, cex = 0.5, col = 'blue')

tmp <- lvl_14[lvl_14$datetime >= as.POSIXct('2014-03-01') & lvl_14$datetime <= as.POSIXct('2014-03-13'),]

par(mfrow = c(2,1), mar = c(2,2,1,1))
plot(tmp$datetime, tmp$level_m, pch = 19, cex = 0.5)
points(tmp$datetime[tmp$derv >= TH], tmp$level_m[tmp$derv >= TH], pch = 19, cex = 0.5, col = 'blue')
lines(tmp$datetime, tmp$smooth)

plot(tmp$datetime, tmp$derv, pch = 19, cex = 0.5)
points(tmp$datetime[tmp$derv >= TH], tmp$derv[tmp$derv >= TH], pch = 19, cex = 0.5, col = 'blue')

tmp <- lvl_14[lvl_14$datetime >= as.POSIXct('2014-10-15') & lvl_14$datetime <= as.POSIXct('2014-11-15'),]

par(mfrow = c(2,1), mar = c(2,2,1,1))
plot(tmp$datetime, tmp$level_m, pch = 19, cex = 0.5)
points(tmp$datetime[tmp$derv >= TH], tmp$level_m[tmp$derv >= TH], pch = 19, cex = 0.5, col = 'blue')
lines(tmp$datetime, tmp$smooth)

plot(tmp$datetime, tmp$derv, pch = 19, cex = 0.5)
points(tmp$datetime[tmp$derv >= TH], tmp$derv[tmp$derv >= TH], pch = 19, cex = 0.5, col = 'blue')

# Looks like an additional filter is needed to prevent a few observations from being flagged

# Also need a filter to remove the situations where observation around missing data are flagged. Work on this first.

tmp <- tmp[order(tmp$datetime),]

tmp$flag <- 0
tmp$flag[tmp$derv >= TH] <- 1

tmp[50:150,c('datetime','level_m','derv','flag')]

# Filter does not work with NA values
r <- rnorm(100)
r[50:55] <- NA
filter(sgd, r)

row <- match(as.POSIXct('2014-10-20 17:00:00', tz = 'MST'), tmp$datetime)
tmp[175:225, c('datetime','level_m','derv','flood_flag')]
# Maybe specify that if more than 4 hrs of data are missing, ignore the 24 hour period preceeding and following the missing data.

# Calculate the time difference between observations
time_gap <- difftime(lvl_14$datetime[-1], lvl_14$datetime[-length(lvl_14$datetime)], units = "hours")

gap_flag <- rep(0, nrow(lvl_14))

for(i in 1:length(time_gap)){
  if(time_gap[i] > 3){
    gap_flag[(i-24):(i+24)] <- 1
    }
}

# So what happens now if the observations close to a gap are ignored?
par(mar = c(4,4,1,1), mfrow = c(1,1))
plot(lvl_14$datetime, lvl_14$level_m, pch = 19, cex = 0.5)
points(lvl_14$datetime[m_derv >= TH & gap_flag == 0], lvl_14$level_m[m_derv >= TH & gap_flag == 0], pch = 19, cex = 0.5, col = 'blue')

# Look at the smaller chunks
lvl_14$gap_flag <- gap_flag
tmp <- lvl_14[lvl_14$datetime >= as.POSIXct('2014-02-25') & lvl_14$datetime <= as.POSIXct('2014-03-05'),]

# Chunk with time gaps in floods
par(mfrow = c(2,1), mar = c(2,2,1,1))
with(tmp, plot(datetime, level_m, pch = 19, cex = 0.5))
with(tmp, points(datetime[derv >= TH & gap_flag == 0], level_m[derv >= TH & gap_flag == 0], pch = 19, cex = 0.5, col = 'blue'))
with(tmp, lines(datetime, smooth))

with(tmp, plot(datetime, level_m, pch = 19, cex = 0.5))
with(tmp, points(datetime[derv >= TH], level_m[derv >= TH], pch = 19, cex = 0.5, col = 'blue'))
with(tmp, lines(datetime, smooth))

# Also no gaps, just sudden jumps in water level
tmp <- lvl_14[lvl_14$datetime >= as.POSIXct('2014-03-01') & lvl_14$datetime <= as.POSIXct('2014-03-13'),]

with(tmp, plot(datetime, level_m, pch = 19, cex = 0.5))
with(tmp, points(datetime[derv >= TH & gap_flag == 0], level_m[derv >= TH & gap_flag == 0], pch = 19, cex = 0.5, col = 'blue'))
with(tmp, lines(datetime, smooth))

with(tmp, plot(datetime, level_m, pch = 19, cex = 0.5))
with(tmp, points(datetime[derv >= TH], level_m[derv >= TH], pch = 19, cex = 0.5, col = 'blue'))
with(tmp, lines(datetime, smooth))

# Find the gap
tmp <- lvl_14[lvl_14$datetime >= as.POSIXct('2014-10-15') & lvl_14$datetime <= as.POSIXct('2014-11-15'),]

with(tmp, plot(datetime, level_m, pch = 19, cex = 0.5))
with(tmp, points(datetime[derv >= TH & gap_flag == 0], level_m[derv >= TH & gap_flag == 0], pch = 19, cex = 0.5, col = 'blue'))
with(tmp, lines(datetime, smooth))

with(tmp, plot(datetime, level_m, pch = 19, cex = 0.5))
with(tmp, points(datetime[derv >= TH], level_m[derv >= TH], pch = 19, cex = 0.5, col = 'blue'))
with(tmp, lines(datetime, smooth))

tmp <- lvl_14[lvl_14$datetime >= as.POSIXct('2014-05-30') & lvl_14$datetime <= as.POSIXct('2014-06-30'),]

with(tmp, plot(datetime, level_m, pch = 19, cex = 0.5))
with(tmp, points(datetime[derv >= TH & gap_flag == 0], level_m[derv >= TH & gap_flag == 0], pch = 19, cex = 0.5, col = 'blue'))
with(tmp, lines(datetime, smooth))

with(tmp, plot(datetime, level_m, pch = 19, cex = 0.5))
with(tmp, points(datetime[derv >= TH], level_m[derv >= TH], pch = 19, cex = 0.5, col = 'blue'))
with(tmp, lines(datetime, smooth))

# How many gaps are there?
difftime(tmp$datetime[nrow(tmp)], tmp$datetime[1], units = "hours")
nrow(tmp)
# So there is 745 hrs included in this period, but only 258 observations

exp <- tmp[tmp$datetime > as.POSIXct("2014-10-22"),]
dplyr::select(exp, datetime, gap_flag)
# My current code only flags the observation with the gap in time, all observations before and after are overwritten with the else term.

# Now require another filter that removes instances where only a few observations are flagged 

# Break up the timeseries into chunks. The chunks will begin when an observation meets the derivative TH and is not flagged for being near missing data. Chunks end when at the first observation that has a negative dervative.

# A possible additional filter is to require more than 3 observations that meet the rising limb flood requirements

i <- 452

lapply(1:nrow(lvl_14), function(i){
  # Find rows that meet the TH requirement
  
  if(lvl_14$derv[i] > TH & lvl_14$gap_flag[i] == 0){
    # Find the peak flow associated with the flood by finding the first observation with a negative derivative
    peak_flow <- with(lvl_14, min(datetime[which(datetime >= datetime[i] & derv < 0)])) - 60*60
    
    # Check if there are more than 3 flagged observations in the set
    check <- nrow(lvl_14[lvl_14$datetime >= lvl_14$datetime[i] & lvl_14$datetime <= peak_flow & lvl_14$derv > TH & lvl_14$gap_flag == 0,]) > 3
    
    # If the check evaluates to TRUE, create a subset of the dataframe that is associated with the flood.
    if(check == T){flood_sub <- lvl_14[lvl_14$datetime >= lvl_14$datetime[i] & lvl_14$datetime <= peak_flow,]}
    
    # Create the next point from which the evaluator should begin next
    next_i <- match(peak_flow, lvl_14$datetime) + 1
    
    # Have to figure out how to change the starting point of the apply function. If it finds a flood, it should skip to the next_i so that I don't get multiple chunk associated with a single flood. But if it doesn't find a flood, it should continue iterating row by row.
    
    # Spoke to Katya. I need to do this by defining my list variables outside of the iterator which then set the conditions for how the iterator is run (skip mode or chunk finder).
    
  }
})

```

