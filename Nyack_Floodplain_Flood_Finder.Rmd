---
title: "Nyack_Floodplain_Flood_Finder"
author: "Amalia Handler"
date: "5/12/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

How to find a flood on a river

Potential things to try
- A threshold associated with the 1st derivative
- Use the anomalize package that was used to detect bad data

Questions/values to help get started
- What are typical 1st derivative values associated with a flood rising limb?
    - The geometric mean is 0.0092
    - The arithmetic mean is 0.0121
- What is the typical window of time used to consider a flood?
    - The rising limb varies between 24 and 60 hours
    - The windows considered so far are 200-400 hours or 8-16 days

```{r}
# The data
level <- read.csv('./Data/Nyack_level_cleaned.csv', header = TRUE, stringsAsFactors = FALSE)

# Remove questionable and bad data
level <- level[-which(level$level_m == -999 | level$level_m_flag == 1 | level$level_m_flag == 2),]

# Convert datetime to POSIX variable type
level$datetime <- as.POSIXct(level$datetime, tz = "MST")

# Rank groundwater wells by distance from the Middle Fork of the Flathead River
# The water level on the Middle Fork (CASC sensor) receives a rank of 1
sensor <- c(6,9,5,7,4,1,3)
rank   <- c(1:7)
rank   <- data.frame(sensor, rank)
level  <- merge(level, rank, by.x = "sensor_number", by.y = "sensor")

# Subset the water level data to only include the sensors for this analysis
level <- level[level$sensor_number %in% sensor,]

# Break the df into a list based on rank
level <- split(level, f = as.factor(level$rank))

# Make the names the sensor number
names(level) <- sensor

# Order each df by the datetime
level <- lapply(level, function(x){
  x[order(x$datetime),]
})

```

Start out by considering one year of data that has floods I've already identified (2014)

Calculate the 1st derivative for the whole time series for the main stem

Flag all instances where the 1st derivative exceeds the threshold (make it so the threshold is easily changable)

```{r}
library(signal)

# Subset to 2014
lvl_14 <- level[[1]][level[[1]]$datetime >= as.POSIXct('2014-01-01') & level[[1]]$datetime <= as.POSIXct('2014-12-31'),]

# Savitsky-Golay filter settings
# p - order of polynomial line fit
# n - number of observations in the window (must be odd)
# m - derivative order to be returned of the fitted line
sg <- sgolay(p = 3, n = 17, m = 0)
sgd <- sgolay(p = 3, n = 17, m = 1)

# Calculate the 1st derivative of the main stem water level
m_derv <- filter(sgd, lvl_14$level_m)

# Set the threshold for flagging obsevations of the first derivative
TH <- 0.0092

# Flag observations over the threshold
lvl_14$flood_flag <- 0
lvl_14$flood_flag[m_derv >= TH] <- 1

# Plot it out
par(mar = c(4,4,1,1), mfrow = c(1,1))
plot(lvl_14$datetime, lvl_14$level_m, pch = 19, cex = 0.5)
points(lvl_14$datetime[m_derv >= TH], lvl_14$level_m[m_derv >= TH], pch = 19, cex = 0.5, col = 'blue')

# An additional filter is required to remove situations where there are observations that are flagged near missing data. 
# Specify that if more than 4 hrs of data are missing, ignore the 24 hour period preceeding and following the missing data.

# Calculate the time difference between observations
time_gap <- difftime(lvl_14$datetime[-1], lvl_14$datetime[-length(lvl_14$datetime)], units = "hours")

gap_flag <- rep(0, nrow(lvl_14))

for(i in 1:length(time_gap)){
  if(time_gap[i] > 3){
    gap_flag[(i-24):(i+24)] <- 1
    }
}

# An additional filter is needed to remove instances where only a few observations are flagged 

# Break up the timeseries into chunks. The chunks will begin when an observation meets the derivative TH and is not flagged for being near missing data. Chunks end when at the first observation that has a negative dervative.

# An additional filter is to require more than 3 observations that meet the rising limb flood requirements

# Set the range of data
r <- 1:nrow(lvl_14)

skip_to <- 1
floods <- lapply(r, function(i){
  before <- (paste('Start:', skip_to))
  row_n <- (paste('Row:', i))

  # Check if criteria are met
  criteria_check <- (lvl_14$derv[i] > TH & lvl_14$gap_flag[i] == 0 & i >= skip_to)
  
  # The chunk finder only runs if the criteria are met
  if(criteria_check){
    # Find the peak flow associated with the flood by finding the first observation with a negative derivative
    peak_flow <- with(lvl_14, min(datetime[which(datetime >= datetime[i] & derv < 0)])) - 60*60
    
    # Subset the flood chunk
    flood_sub <- lvl_14[lvl_14$datetime >= lvl_14$datetime[i] &
                            lvl_14$datetime <= peak_flow,]
    
    # Within the subset, check if more than 3 observations meet the criteria
    min_obs_check <- nrow(flood_sub[flood_sub$derv > TH & flood_sub$gap_flag == 0,]) > 3
    
    if(min_obs_check == F){flood_sub <- NULL}

    # Create the next point from which the evaluator should begin next
    next_i <- match(peak_flow, lvl_14$datetime) + 1
    
    # Assign the next row to start the check process to the global environment
    assign("skip_to", next_i, envir = .GlobalEnv)
  } else {flood_sub <- NULL}

  after <- (paste('End:', skip_to))
  print(paste(row_n, before, after, sep = ', '))
  return(flood_sub)
})

# Remove all null objects from the list, only retaining objects that contain floods
floods <- floods[-which(sapply(floods, is.null))]

# Get the start date for each of the floods identified
flood_first_date <- lapply(floods, function(x){
  x$datetime[1]
})

# Plot the time series

# Use to alter the range of observations plotted
r <- 7000:nrow(lvl_14)

par(mfrow = c(1,1))
# Plot the raw observations
plot(lvl_14$datetime[r], lvl_14$level_m[r], pch = 19, cex = 0.5)
# Highlight points flagged for being above the 1st derivative TH and not flagged for being near missing observations
with(lvl_14[r,], points(datetime[derv > TH & gap_flag == 0], level_m[derv > TH & gap_flag == 0], pch = 19, cex = 0.5, col = 'blue'))
# Add the smoothed line for the interpolation
with(lvl_14[r,], lines(datetime, smooth))
# Add vertical bars at the start of floods identified in the algorithm
lapply(flood_first_date, function(x){abline(v = x)})

```

